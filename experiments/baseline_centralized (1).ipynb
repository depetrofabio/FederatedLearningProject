{"cells":[{"cell_type":"code","execution_count":null,"id":"228da2b7","metadata":{"id":"228da2b7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746623425415,"user_tz":-120,"elapsed":32740,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"e93c5dd1-ddef-4352-c0bc-0f666c675606"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import numpy as np\n","import wandb\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import shutil\n","import os                              # Import the 'os' module for changing directories\n","os.chdir('/content/drive/MyDrive/FL')  # Change the directory"]},{"cell_type":"code","execution_count":null,"id":"f27d1321","metadata":{"id":"f27d1321"},"outputs":[],"source":["import torch\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms\n","from torchvision.datasets import CIFAR100\n","from torch.utils.data import Subset, DataLoader, random_split\n","\n","from FederatedLearningProject.data.cifar100_loader import get_cifar100\n","import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n","from FederatedLearningProject.training.centralized_training import train_and_validate"]},{"cell_type":"code","execution_count":null,"id":"usCBOVfNjyQI","metadata":{"id":"usCBOVfNjyQI","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1746623463992,"user_tz":-120,"elapsed":25108,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"eda784ee-f83f-4cf5-b29e-c83ca00c4671"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcerbellifederico\u001b[0m (\u001b[33mcerbellifederico-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}],"source":["wandb.login() # Ask for your API key for logging in to the wandb library."]},{"cell_type":"code","execution_count":null,"id":"f1uwQJOSKHgl","metadata":{"id":"f1uwQJOSKHgl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746623476351,"user_tz":-120,"elapsed":12360,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"203fc4e5-0cd8-49d8-b632-7d25d4b89532"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images in Training Set:   40000\n","Number of images in Validation Set: 10000\n","Number of images in Test Set:       10000\n","✅ Datasets loaded successfully\n"]}],"source":["# Import CIFAR100 dataset: train_set, val_set, test_set\n","# The transforms are applied before returning the dataset (in the module)\n","\n","valid_split_perc = 0.2    # of the 50000 training data\n","train_set, val_set, test_set = get_cifar100(valid_split_perc)"]},{"cell_type":"code","execution_count":null,"id":"c_UDv-rPjVSj","metadata":{"id":"c_UDv-rPjVSj"},"outputs":[],"source":["# Create DataLoaders for training, validation, and test sets\n","\n","# batch_size è in hyperparameter (64, 128, ..), anche num_workers (consigliato per colab 2 o 4)\n","\n","train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)"]},{"cell_type":"markdown","id":"TaKru93QF_8C","metadata":{"id":"TaKru93QF_8C"},"source":["\n","### Possible Models\n","|                        | **Simple linear head**                               | **MLP head w/ Dropout**                                                      |\n","| :--------------------- | :--------------------------------------------------- | :--------------------------------------------------------------------------- |\n","| **Definition**         | `nn.Linear(384 → 100)`                               | `Dropout → Linear(384 → 256) → ReLU → Dropout → Linear(256 → 100)`           |\n","| **# trainable params** | 384×100 + 100 ≈ **38 500**                           | 384×256+256 + 256×100+100 ≈ **123 000**                                      |\n","| **Regularization**     | none                                                 | dropout on both layers                                                       |\n","| **Expressive power**   | low  – just a single hyperplane on the CLS embedding | higher – small nonlinear bottleneck can learn more complex features in heads |\n","| **Compute / memory**   | minimal                                              | \\~3× more weights, a bit more forward/backward cost                          |\n","\n","---\n","\n","**Appunto sui layer di testa:**\n","\n","1. **`self.classifier`**\n","\n","   * **Cosa contiene?** Un singolo `nn.Linear(embed_dim → num_classes)`.\n","   * **Quando usarlo?** Se vuoi un *linear probe* puro: un solo layer che prende il CLS token e mappa direttamente alle classi.\n","   * **Pro:** estremamente leggero (∼38 K parametri), veloce da addestrare e da inferire.\n","   * **Contro:** capacità espressiva minima (è solo un’iper‐superficie lineare sullo spazio degli embedding).\n","\n","2. **`self.head`**\n","\n","   * **Cosa contiene?** Una piccola sequenza (`nn.Sequential`) di layer:\n","\n","     * Dropout\n","     * Linear (embed\\_dim → hidden\\_dim)\n","     * ReLU\n","     * Dropout\n","     * Linear (hidden\\_dim → num\\_classes)\n","   * **Quando usarlo?** Se vuoi dare al tuo “probe” un po’ più di potenza di calcolo, trasformando non-linearmente il CLS prima della classificazione.\n","   * **Pro:** maggiore capacità di apprendere rappresentazioni complesse nella testa, un minimo di regolarizzazione via dropout.\n","   * **Contro:** più pesante (∼3× parametri in più rispetto al solo `classifier`), leggermente più lento da addestrare e inferire.\n","\n","---\n","\n","### Perché una piuttosto che l’altra?\n","\n","* **Vincoli di risorse** (GPU/RAM, tempo d’addestramento):\n","\n","  * Se sei sotto forte pressione computazionale o vuoi risultati rapidi, opti per `self.classifier`.\n","* **Prestazioni** (accuratezza su dataset piccolo/mediamente grande come CIFAR-100):\n","\n","  * Se noti che il linear probe raggiunge un plateau basso, un piccolo MLP (`self.head`) può guadagnare qualche punto percentuale in più.\n","* **Semplicità vs flessibilità**:\n","\n","  * Con una sola `classifier` hai un codice più pulito e diretto.\n","  * Con `head` puoi sperimentare — cambiare `hidden_dim`, aggiungere altro dropout, batchnorm o ulteriori layer.\n","\n","In definitiva, **il nome** (`classifier` vs `head`) è arbitrario: serve a rendere più chiaro nel codice di che “peso” stiamo parlando. Se hai un solo layer, chiamalo `classifier`; se invece è un blocco più articolato, chiamalo `head` o `projection_head`, per tener separata la parte “feature extractor” (backbone) dalla parte “feature consumer” (testa di classificazione).\n"]},{"cell_type":"code","execution_count":null,"id":"jRRs8T8qCwcr","metadata":{"id":"jRRs8T8qCwcr"},"outputs":[],"source":["# # --- MLP head w/ Dropout ---\n","# # Load DINO ViT-S/16 backbone and freeze it\n","# backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n","# backbone.eval()\n","\n","# for p in backbone.parameters():\n","#     p.requires_grad = False\n","\n","# # Define the classifier head with optional dropout/MLP\n","# class DinoClassifier(nn.Module):\n","#     def __init__(self, backbone, num_classes=100, hidden_dim=256, drop=0.5):\n","#         super().__init__()\n","#         self.backbone = backbone\n","#         embed_dim = backbone.embed_dim  # 384 for ViT-S/16\n","#         self.classifier = nn.Sequential(\n","#             nn.Dropout(drop),\n","#             nn.Linear(embed_dim, hidden_dim),\n","#             nn.ReLU(inplace=True),\n","#             nn.Dropout(drop),\n","#             nn.Linear(hidden_dim, num_classes)\n","#         )\n","\n","#     def forward(self, x):\n","#         # estraggo il CLS token dall'unico layer intermedio\n","#         with torch.no_grad():\n","#             # restituisce lista di n tensor [B, 1+N, embed_dim]\n","#             feats = self.backbone.get_intermediate_layers(x, n=1)[0]\n","#         cls = feats[:, 0]   # prendo solo il token [CLS]\n","#         return self.classifier(cls)\n","\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = DinoClassifier(backbone, num_classes=100).to(device)\n","\n","# # ensure backbone stays in eval, head in train\n","# model.backbone.eval()\n","# model.classifier.train()"]},{"cell_type":"code","source":["# # Freeze only the first 9 blocks of the ViT backbone\n","# backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n","# for i, block in enumerate(backbone.blocks):\n","#     if i < 9:\n","#         for p in block.parameters():\n","#             p.requires_grad = False\n","\n","# ] Freeze patch embedding and dropout\n","# for p in backbone.patch_embed.parameters():\n","#     p.requires_grad = False\n","\n","# for p in backbone.pos_drop.parameters():\n","#     p.requires_grad = False\n","\n","# # Define the classifier head with optional dropout/MLP\n","# class DinoClassifier(nn.Module):\n","#     def __init__(self, backbone, num_classes=100, hidden_dim=256, drop=0.5):\n","#         super().__init__()\n","#         self.backbone = backbone\n","#         embed_dim = backbone.embed_dim  # 384 for ViT-S/16\n","#         self.classifier = nn.Sequential(\n","#             nn.Dropout(drop),\n","#             nn.Linear(embed_dim, hidden_dim),\n","#             nn.ReLU(inplace=True),\n","#             nn.Dropout(drop),\n","#             nn.Linear(hidden_dim, num_classes)\n","#         )\n","\n","#     def forward(self, x):\n","#         feats = self.backbone.get_intermediate_layers(x, n=1)[0]\n","#         cls = feats[:, 0]\n","#         return self.classifier(cls)\n","\n","# # Instantiate model and move to device\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = DinoClassifier(backbone, num_classes=100).to(device)\n","\n","# # Set backbone to train mode (so dropout works during training)\n","# model.backbone.train()  # Ensure backbone is in training mode\n","# model.classifier.train()  # Keep classifier in train mode as well\n","\n"],"metadata":{"id":"WKtFc5PxZXB3"},"id":"WKtFc5PxZXB3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"rLLuc8n9E92o","metadata":{"id":"rLLuc8n9E92o"},"outputs":[],"source":["# # --- Simple linear Head ---\n","# # Load DINO ViT-S/16 backbone e freeze\n","# backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n","# backbone.eval()\n","\n","# for p in backbone.parameters():\n","#     p.requires_grad = False\n","\n","# # Define the classifier head\n","# class DinoClassifier(nn.Module):\n","#     def __init__(self, backbone, num_classes=100):\n","#         super().__init__()\n","#         self.backbone = backbone\n","#         embed_dim = backbone.embed_dim  # 384 per ViT-S/16\n","#         self.classifier = nn.Linear(embed_dim, num_classes)\n","\n","#     def forward(self, x):\n","#         # estraggo il CLS token dall'unico layer intermedio\n","#         with torch.no_grad():\n","#             # restituisce lista di n tensor [B, 1+N, embed_dim]\n","#             feats = self.backbone.get_intermediate_layers(x, n=1)[0]\n","#         cls = feats[:, 0]   # prendo solo il token [CLS]\n","#         return self.classifier(cls)\n","\n","# model = DinoClassifier(backbone, num_classes=100)\n","\n","# # Device selection: Use GPU if available, otherwise CPU\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# model = model.to(device)"]},{"cell_type":"code","source":["# --- Simple linear Head ---\n","# Load DINO ViT-S/16 backbone e freeze\n","backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n","\n","for i, block in enumerate(backbone.blocks):\n","    if i < 9:\n","        for p in block.parameters():\n","            p.requires_grad = False\n","\n","# Freeze patch embedding and dropout\n","for p in backbone.patch_embed.parameters():\n","    p.requires_grad = False\n","\n","for p in backbone.pos_drop.parameters():\n","    p.requires_grad = False\n","\n","# Define the classifier head\n","class DinoClassifier(nn.Module):\n","    def __init__(self, backbone, num_classes=100):\n","        super().__init__()\n","        self.backbone = backbone\n","        embed_dim = backbone.embed_dim  # 384 per ViT-S/16\n","        self.classifier = nn.Linear(embed_dim, num_classes)\n","\n","    def forward(self, x):\n","        # estraggo il CLS token dall'unico layer intermedio\n","        feats = self.backbone.get_intermediate_layers(x, n=1)[0]\n","        cls = feats[:, 0]   # prendo solo il token [CLS]\n","        return self.classifier(cls)\n","\n","model = DinoClassifier(backbone, num_classes=100)\n","\n","# Device selection: Use GPU if available, otherwise CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = model.to(device)\n","\n","model.backbone.train()  # Ensure backbone is in training mode\n","model.classifier.train()  # Keep classifier in train mode as well"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ihi72n2baQH2","executionInfo":{"status":"ok","timestamp":1746623479408,"user_tz":-120,"elapsed":2971,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"bf98b492-c470-4fb0-e145-8dd187ff79af"},"id":"Ihi72n2baQH2","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/facebookresearch/dino/zipball/main\" to /root/.cache/torch/hub/main.zip\n","Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth\n","100%|██████████| 82.7M/82.7M [00:00<00:00, 159MB/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=384, out_features=100, bias=True)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":null,"id":"x3NlXi1Y-PB2","metadata":{"id":"x3NlXi1Y-PB2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746623479413,"user_tz":-120,"elapsed":3,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"a2d4a522-29d6-4a95-c077-802a0827cd32"},"outputs":[{"output_type":"stream","name":"stdout","text":["cpu\n"]}],"source":["print(device)"]},{"cell_type":"code","execution_count":null,"id":"A-GWt2WqlJyY","metadata":{"id":"A-GWt2WqlJyY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746623479438,"user_tz":-120,"elapsed":24,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"5d413219-85c7-439c-b251-80330a101707"},"outputs":[{"output_type":"stream","name":"stdout","text":["DinoClassifier(\n","  (backbone): VisionTransformer(\n","    (patch_embed): PatchEmbed(\n","      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","    )\n","    (pos_drop): Dropout(p=0.0, inplace=False)\n","    (blocks): ModuleList(\n","      (0-11): 12 x Block(\n","        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (attn): Attention(\n","          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=384, out_features=384, bias=True)\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","    (head): Identity()\n","  )\n","  (classifier): Linear(in_features=384, out_features=100, bias=True)\n",")\n"]}],"source":["print(model)"]},{"cell_type":"code","execution_count":null,"id":"RqiWVe89NEJC","metadata":{"id":"RqiWVe89NEJC"},"outputs":[],"source":["# --- OPTIMIZER AND LOSS FUNCTION ---\n","learning_rate = 1e-4\n","momentum = 0.9\n","weight_decay = 5e-5\n","epochs = 50\n","\n","optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"id":"w2WnTA_ue2sX","metadata":{"id":"w2WnTA_ue2sX","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1746623482086,"user_tz":-120,"elapsed":2603,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"27606521-aa69-4bb3-b24d-8af81e6e5c93"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.10"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/FL/wandb/run-20250507_131118-6mmze27n</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n' target=\"_blank\">dino_vits16_run</a></strong> to <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject' target=\"_blank\">https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n' target=\"_blank\">https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n</a>"]},"metadata":{}}],"source":["# wandb.init() prepares the tracking of hyperparameters/metrics for later recording performance using wandb.log()\n","\n","model_name = \"dino_vits16\"\n","project_name = \"FederatedProject\"\n","run_name = f\"{model_name}_run\"\n","\n","# INITIALIZE W&B\n","wandb.init(\n","    project=project_name,\n","    name=run_name,\n","    config={\n","        \"model\": \"dino_vits16\",\n","        \"epochs\": epochs,\n","        \"batch_size\": train_loader.batch_size,\n","        \"learning_rate\": learning_rate,  # Use fixed value\n","        \"weight_decay\": weight_decay,\n","        \"momentum\": momentum,\n","        \"architecture\": model.__class__.__name__,\n","    }\n",")\n","\n","# Copy your config\n","config = wandb.config\n"]},{"cell_type":"code","execution_count":null,"id":"mo4lpz6c3C-F","metadata":{"id":"mo4lpz6c3C-F"},"outputs":[],"source":["#  PERCORSO CHECKPOINT\n","checkpoint_dir = \"/content/drive/MyDrive/FL/FederatedLearningProject/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_checkpoint.pth\")    # we predefine the name of the file inside the specified folder (dir)"]},{"cell_type":"code","execution_count":null,"id":"cBdBMnxzyusN","metadata":{"id":"cBdBMnxzyusN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746623482130,"user_tz":-120,"elapsed":18,"user":{"displayName":"Federico Cerbelli","userId":"14904905765922533529"}},"outputId":"bf1bfeff-5f53-426a-ef5c-453bfa855275"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Nessun checkpoint trovato, inizio da epoca 1.\n","\n"]}],"source":["# RECOVER CHECKPOINT\n","start_epoch, model_data = checkpointing.load_checkpoint(model, optimizer, checkpoint_dir)\n","\n","try:\n","  print()\n","  print(f\"The 'model_data' dictionary contains the following keys: {list(model_data.keys())}\")\n","  model.load_state_dict(model_data[\"model_state_dict\"])\n","  optimizer.load_state_dict(model_data[\"optimizer_state_dict\"])\n","except: None\n","\n"]},{"cell_type":"code","execution_count":null,"id":"3ZGaA_bF5Cpy","metadata":{"id":"3ZGaA_bF5Cpy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"765125ef-2d0e-4801-b157-d4116b1cc8f7"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[Epoch 1] Train Loss: 3.8797, Train Accuracy: 13.80%, Val Loss: 2.6817, Val Accuracy: 32.51%\n","[Epoch 2] Train Loss: 2.3888, Train Accuracy: 37.88%, Val Loss: 2.1358, Val Accuracy: 42.80%\n"]}],"source":["# --- TRAINING LOOP ---\n","# Call to the training loop function\n","train_and_validate(start_epoch, model, train_loader, val_loader, optimizer, criterion, device, checkpoint_path, num_epochs=50, checkpoint_interval=10)"]},{"cell_type":"code","execution_count":null,"id":"lCT7l21Lpl9L","metadata":{"id":"lCT7l21Lpl9L"},"outputs":[],"source":["## Display some informations ##\n","\n","print(\"Model:\", model_name)\n","print(\"Train set size:\", len(train_set))\n","print(\"Validation set size:\", len(val_set))\n","print(\"Batch size:\", train_loader.batch_size)\n","print(\"Number of epochs:\", config.epochs)\n","print(\"DataLoader: \")\n","print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n","print(\"Architecture:\", model.__class__.__name__)\n","print(\"Device:\", device)\n","print(\"Optimizer:\", optimizer)\n","print(\"Loss function:\", criterion)\n","print(\"Checkpoint directory:\", checkpoint_dir)\n","print(\"Checkpoint path:\", checkpoint_path)\n","print(\"Current epoch:\", epoch)\n","print()\n","\n","print(\"Train Loader Information:\")\n","print(f\"  Number of batches: {len(train_loader)}\")\n","print(f\"  Batch size: {train_loader.batch_size}\")\n","# Get the dimension of a single batch\n","for images, labels in train_loader:\n","  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n","  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n","  break  # Exit the loop after processing one batch\n","print()\n","\n","print(\"\\nValidation Loader Information:\")\n","print(f\"  Number of batches: {len(val_loader)}\")\n","print(f\"  Batch size: {val_loader.batch_size}\")\n","# Get the dimension of a single batch\n","for images, labels in val_loader:\n","  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n","  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n","  break  # Exit the loop after processing one batch\n","print()\n","\n","# Check for CUDA availability\n","print(\"CUDA AVAIABILITY:\")\n","if torch.cuda.is_available():\n","    print(\"CUDA is available. Using GPU.\")\n","    print(\"Number of GPUs:\", torch.cuda.device_count())\n","    print(\"Current GPU:\", torch.cuda.current_device())\n","    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","else:\n","    print(\"CUDA is not available. Using CPU.\")\n","\n","# Print model architecture summary\n","print(\"\\nMODEL ARCHITECTURE:\")\n","print(model)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}