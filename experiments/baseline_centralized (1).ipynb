{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228da2b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "228da2b7",
        "outputId": "e93c5dd1-ddef-4352-c0bc-0f666c675606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import shutil\n",
        "import os                              # Import the 'os' module for changing directories\n",
        "os.chdir('/content/drive/MyDrive/FL')  # Change the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f27d1321",
      "metadata": {
        "id": "f27d1321"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "\n",
        "from FederatedLearningProject.data.cifar100_loader import get_cifar100\n",
        "import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n",
        "from FederatedLearningProject.training.centralized_training import train_and_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "usCBOVfNjyQI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "usCBOVfNjyQI",
        "outputId": "eda784ee-f83f-4cf5-b29e-c83ca00c4671"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcerbellifederico\u001b[0m (\u001b[33mcerbellifederico-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login() # Ask for your API key for logging in to the wandb library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1uwQJOSKHgl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1uwQJOSKHgl",
        "outputId": "203fc4e5-0cd8-49d8-b632-7d25d4b89532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of images in Training Set:   40000\n",
            "Number of images in Validation Set: 10000\n",
            "Number of images in Test Set:       10000\n",
            "✅ Datasets loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Import CIFAR100 dataset: train_set, val_set, test_set\n",
        "# The transforms are applied before returning the dataset (in the module)\n",
        "\n",
        "valid_split_perc = 0.2    # of the 50000 training data\n",
        "train_set, val_set, test_set = get_cifar100(valid_split_perc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c_UDv-rPjVSj",
      "metadata": {
        "id": "c_UDv-rPjVSj"
      },
      "outputs": [],
      "source": [
        "# Create DataLoaders for training, validation, and test sets\n",
        "\n",
        "# batch_size è in hyperparameter (64, 128, ..), anche num_workers (consigliato per colab 2 o 4)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=128, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_set, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TaKru93QF_8C",
      "metadata": {
        "id": "TaKru93QF_8C"
      },
      "source": [
        "\n",
        "### Possible Models\n",
        "|                        | **Simple linear head**                               | **MLP head w/ Dropout**                                                      |\n",
        "| :--------------------- | :--------------------------------------------------- | :--------------------------------------------------------------------------- |\n",
        "| **Definition**         | `nn.Linear(384 → 100)`                               | `Dropout → Linear(384 → 256) → ReLU → Dropout → Linear(256 → 100)`           |\n",
        "| **# trainable params** | 384×100 + 100 ≈ **38 500**                           | 384×256+256 + 256×100+100 ≈ **123 000**                                      |\n",
        "| **Regularization**     | none                                                 | dropout on both layers                                                       |\n",
        "| **Expressive power**   | low  – just a single hyperplane on the CLS embedding | higher – small nonlinear bottleneck can learn more complex features in heads |\n",
        "| **Compute / memory**   | minimal                                              | \\~3× more weights, a bit more forward/backward cost                          |\n",
        "\n",
        "---\n",
        "\n",
        "**Appunto sui layer di testa:**\n",
        "\n",
        "1. **`self.classifier`**\n",
        "\n",
        "   * **Cosa contiene?** Un singolo `nn.Linear(embed_dim → num_classes)`.\n",
        "   * **Quando usarlo?** Se vuoi un *linear probe* puro: un solo layer che prende il CLS token e mappa direttamente alle classi.\n",
        "   * **Pro:** estremamente leggero (∼38 K parametri), veloce da addestrare e da inferire.\n",
        "   * **Contro:** capacità espressiva minima (è solo un’iper‐superficie lineare sullo spazio degli embedding).\n",
        "\n",
        "2. **`self.head`**\n",
        "\n",
        "   * **Cosa contiene?** Una piccola sequenza (`nn.Sequential`) di layer:\n",
        "\n",
        "     * Dropout\n",
        "     * Linear (embed\\_dim → hidden\\_dim)\n",
        "     * ReLU\n",
        "     * Dropout\n",
        "     * Linear (hidden\\_dim → num\\_classes)\n",
        "   * **Quando usarlo?** Se vuoi dare al tuo “probe” un po’ più di potenza di calcolo, trasformando non-linearmente il CLS prima della classificazione.\n",
        "   * **Pro:** maggiore capacità di apprendere rappresentazioni complesse nella testa, un minimo di regolarizzazione via dropout.\n",
        "   * **Contro:** più pesante (∼3× parametri in più rispetto al solo `classifier`), leggermente più lento da addestrare e inferire.\n",
        "\n",
        "---\n",
        "\n",
        "### Perché una piuttosto che l’altra?\n",
        "\n",
        "* **Vincoli di risorse** (GPU/RAM, tempo d’addestramento):\n",
        "\n",
        "  * Se sei sotto forte pressione computazionale o vuoi risultati rapidi, opti per `self.classifier`.\n",
        "* **Prestazioni** (accuratezza su dataset piccolo/mediamente grande come CIFAR-100):\n",
        "\n",
        "  * Se noti che il linear probe raggiunge un plateau basso, un piccolo MLP (`self.head`) può guadagnare qualche punto percentuale in più.\n",
        "* **Semplicità vs flessibilità**:\n",
        "\n",
        "  * Con una sola `classifier` hai un codice più pulito e diretto.\n",
        "  * Con `head` puoi sperimentare — cambiare `hidden_dim`, aggiungere altro dropout, batchnorm o ulteriori layer.\n",
        "\n",
        "In definitiva, **il nome** (`classifier` vs `head`) è arbitrario: serve a rendere più chiaro nel codice di che “peso” stiamo parlando. Se hai un solo layer, chiamalo `classifier`; se invece è un blocco più articolato, chiamalo `head` o `projection_head`, per tener separata la parte “feature extractor” (backbone) dalla parte “feature consumer” (testa di classificazione).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WKtFc5PxZXB3",
      "metadata": {
        "id": "WKtFc5PxZXB3",
        "outputId": "e25ab85d-21b1-41d4-c5d9-2f31146051ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.5, inplace=False)\n",
              "  (1): Linear(in_features=384, out_features=256, bias=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): Dropout(p=0.5, inplace=False)\n",
              "  (4): Linear(in_features=256, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Freeze only the first 9 blocks of the ViT backbone\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
        "\n",
        "# Freeze patch embedding and dropout\n",
        "for p in backbone.patch_embed.parameters():  # embedding vectors sono rappresentazioni numeriche dei dati\n",
        "    p.requires_grad = False\n",
        "\n",
        "backbone.pos_embed.requires_grad = False\n",
        "backbone.cls_token.requires_grad = False\n",
        "\n",
        "\"\"\"\n",
        "pos_drop is likely an nn.Dropout layer associated with positional embeddings. nn.Dropout layers themselves don't have learnable parameters\n",
        "that are updated during backpropagation (only a dropout rate, which is a hyperparameter). So, iterating through parameters() of a standard\n",
        " dropout layer might yield an empty iterator or no parameters that gradients flow through. The main control over dropout is its train() or\n",
        " eval() mode. However, if pos_drop were a custom module with learnable parameters, this would freeze them. This line is unlikely to cause\n",
        " issues but might not have a significant effect if pos_drop is a standard nn.Dropout.\n",
        "\n",
        "for p in backbone.pos_drop.parameters():     # da verificare, non dovrebbero esserci parametri trainabili nei drop out layers\n",
        "    p.requires_grad = False\n",
        "\"\"\"\n",
        "\n",
        "# Define the classifier head with optional dropout/MLP\n",
        "class DinoClassifier(nn.Module):\n",
        "    def __init__(self, backbone, num_classes=100, hidden_dim=256, drop=0.5): # hidden_dim = dimensione del layer della nn\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        embed_dim = backbone.embed_dim  # 384 for ViT-S/16\n",
        "        self.classifier = nn.Sequential(\n",
        "            # nn.Dropout(drop),                # solitamnete non si fa il dropout prima dell'input layer, da capire\n",
        "            nn.Linear(embed_dim, hidden_dim),     # from 384 to 256\n",
        "            nn.ReLU(inplace=True), # capire meglio inplace\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(hidden_dim, num_classes)    # from 256 to 100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats = self.backbone.get_intermediate_layers(x, n=1)[0] # take the output features from DiNo's backbone\n",
        "        cls = feats[:, 0]                                        #\n",
        "        return self.classifier(cls)\n",
        "\n",
        "# Instantiate model and move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = DinoClassifier(backbone, num_classes=100).to(device)\n",
        "\n",
        "# Freeze first 9 blocks\n",
        "for block in model.backbone.blocks[0:9]:\n",
        "    block.eval()\n",
        "    for param in block.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Unfreeze remaining blocks (if needed)\n",
        "for block in model.backbone.blocks[9:]:\n",
        "    block.train()\n",
        "    for param in block.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "model.classifier.train()\n",
        "\n",
        "# Set backbone to train mode (so dropout works during training)\n",
        "# model.backbone.train()  # Ensure backbone is in training mode, non dovrebbe modificare i blocchi freezzati\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def debug_model(model: nn.Module, model_name: str = \"Model\"):\n",
        "    \"\"\"\n",
        "    Prints debugging information about a PyTorch model.\n",
        "\n",
        "    Information includes:\n",
        "    - Overall device of the first parameter (indicative of model's primary device).\n",
        "    - For each named parameter:\n",
        "        - Full parameter name.\n",
        "        - Device of the parameter.\n",
        "        - Whether the parameter requires gradients (is frozen or not).\n",
        "        - Inferred block index if the name matches a ViT-like structure.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Debugging {model_name} ---\")\n",
        "\n",
        "    # Check overall model device (based on the first parameter)\n",
        "    try:\n",
        "        first_param_device = next(model.parameters()).device\n",
        "        print(f\"{model_name} is primarily on device: {first_param_device}\")\n",
        "    except StopIteration:\n",
        "        print(f\"{model_name} has no parameters.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nParameter Details (Name | Device | Requires Grad? | Inferred Block):\")\n",
        "    for name, param in model.named_parameters():\n",
        "        device = param.device\n",
        "        requires_grad = param.requires_grad\n",
        "\n",
        "        block_info = \"N/A\"\n",
        "        # Try to infer block index for ViT-like models\n",
        "        if \"blocks.\" in name:\n",
        "            try:\n",
        "                # e.g., name = \"blocks.0.attn.qkv.weight\"\n",
        "                block_idx_str = name.split(\"blocks.\")[1].split(\".\")[0]\n",
        "                if block_idx_str.isdigit():\n",
        "                    block_info = f\"Block {block_idx_str}\"\n",
        "            except IndexError:\n",
        "                block_info = \"Block (parse error)\"\n",
        "\n",
        "        print(f\"- {name:<50} | {str(device):<10} | {str(requires_grad):<15} | {block_info}\")\n",
        "\n",
        "    # You can add more specific checks here, e.g., for model mode (train/eval)\n",
        "    print(f\"{model_name} is in {'training' if model.training else 'evaluation'} mode.\")\n",
        "    print(f\"--- End Debugging {model_name} ---\\n\")\n",
        ""
      ],
      "metadata": {
        "id": "y-h5o7y1CQHP"
      },
      "id": "y-h5o7y1CQHP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "debug_model(model=model)"
      ],
      "metadata": {
        "id": "DpbqVvF7DRCC"
      },
      "id": "DpbqVvF7DRCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x3NlXi1Y-PB2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3NlXi1Y-PB2",
        "outputId": "a2d4a522-29d6-4a95-c077-802a0827cd32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A-GWt2WqlJyY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-GWt2WqlJyY",
        "outputId": "5d413219-85c7-439c-b251-80330a101707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DinoClassifier(\n",
            "  (backbone): VisionTransformer(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (blocks): ModuleList(\n",
            "      (0-11): 12 x Block(\n",
            "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (attn): Attention(\n",
            "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
            "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (drop_path): Identity()\n",
            "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "        (mlp): Mlp(\n",
            "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "          (drop): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "    (head): Identity()\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=384, out_features=256, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=256, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RqiWVe89NEJC",
      "metadata": {
        "id": "RqiWVe89NEJC"
      },
      "outputs": [],
      "source": [
        "# --- OPTIMIZER AND LOSS FUNCTION ---\n",
        "learning_rate = 1e-4\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-5\n",
        "epochs = 50\n",
        "\n",
        "\"\"\"\n",
        "# Example for differential learning rates:\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.backbone.blocks[9:].parameters(), 'lr': 1e-5}, # Adjust block indices if needed\n",
        "    # You might also want to fine-tune backbone.norm if it exists and is not frozen\n",
        "    # {'params': model.backbone.norm.parameters(), 'lr': 1e-5},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
        "], weight_decay=0.05) # example weight decay\n",
        "\"\"\"\n",
        "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
        "# Example optimizer instantiation:\n",
        "optimizer = torch.optim.SGD(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=learning_rate, # Example LR\n",
        "    weight_decay=weight_decay,\n",
        "    momentum=momentum\n",
        ")\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "w2WnTA_ue2sX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "w2WnTA_ue2sX",
        "outputId": "27606521-aa69-4bb3-b24d-8af81e6e5c93"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/FL/wandb/run-20250507_131118-6mmze27n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n' target=\"_blank\">dino_vits16_run</a></strong> to <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject' target=\"_blank\">https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n' target=\"_blank\">https://wandb.ai/cerbellifederico-politecnico-di-torino/FederatedProject/runs/6mmze27n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# wandb.init() prepares the tracking of hyperparameters/metrics for later recording performance using wandb.log()\n",
        "\n",
        "model_name = \"dino_vits16\"\n",
        "project_name = \"FederatedProject\"\n",
        "run_name = f\"{model_name}_run\"\n",
        "\n",
        "# INITIALIZE W&B\n",
        "wandb.init(\n",
        "    project=project_name,\n",
        "    name=run_name,\n",
        "    config={\n",
        "        \"model\": \"dino_vits16\",\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": train_loader.batch_size,\n",
        "        \"learning_rate\": learning_rate,  # Use fixed value\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"momentum\": momentum,\n",
        "        \"architecture\": model.__class__.__name__,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Copy your config\n",
        "config = wandb.config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mo4lpz6c3C-F",
      "metadata": {
        "id": "mo4lpz6c3C-F"
      },
      "outputs": [],
      "source": [
        "#  PERCORSO CHECKPOINT\n",
        "checkpoint_dir = \"/content/drive/MyDrive/FL/FederatedLearningProject/checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_checkpoint.pth\")    # we predefine the name of the file inside the specified folder (dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cBdBMnxzyusN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBdBMnxzyusN",
        "outputId": "bf1bfeff-5f53-426a-ef5c-453bfa855275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Nessun checkpoint trovato, inizio da epoca 1.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# RECOVER CHECKPOINT\n",
        "start_epoch, model_data = checkpointing.load_checkpoint(model, optimizer, checkpoint_dir)\n",
        "\n",
        "try:\n",
        "  print()\n",
        "  print(f\"The 'model_data' dictionary contains the following keys: {list(model_data.keys())}\")\n",
        "  model.load_state_dict(model_data[\"model_state_dict\"])\n",
        "  optimizer.load_state_dict(model_data[\"optimizer_state_dict\"])\n",
        "except: None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZGaA_bF5Cpy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZGaA_bF5Cpy",
        "outputId": "765125ef-2d0e-4801-b157-d4116b1cc8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 3.8797, Train Accuracy: 13.80%, Val Loss: 2.6817, Val Accuracy: 32.51%\n",
            "[Epoch 2] Train Loss: 2.3888, Train Accuracy: 37.88%, Val Loss: 2.1358, Val Accuracy: 42.80%\n"
          ]
        }
      ],
      "source": [
        "# --- TRAINING LOOP ---\n",
        "# Call to the training loop function\n",
        "train_and_validate(start_epoch, model=model, train=train_loader, val_loader=val_loader, scheduler=scheduler, optimizer=optimizer, criterion=criterion, device=device, checkpoint_path=checkpoint_path, num_epochs=50, checkpoint_interval=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lCT7l21Lpl9L",
      "metadata": {
        "id": "lCT7l21Lpl9L"
      },
      "outputs": [],
      "source": [
        "## Display some informations ##\n",
        "\n",
        "print(\"Model:\", model_name)\n",
        "print(\"Train set size:\", len(train_set))\n",
        "print(\"Validation set size:\", len(val_set))\n",
        "print(\"Batch size:\", train_loader.batch_size)\n",
        "print(\"Number of epochs:\", config.epochs)\n",
        "print(\"DataLoader: \")\n",
        "print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n",
        "print(\"Architecture:\", model.__class__.__name__)\n",
        "print(\"Device:\", device)\n",
        "print(\"Optimizer:\", optimizer)\n",
        "print(\"Loss function:\", criterion)\n",
        "print(\"Checkpoint directory:\", checkpoint_dir)\n",
        "print(\"Checkpoint path:\", checkpoint_path)\n",
        "print(\"Current epoch:\", epoch)\n",
        "print()\n",
        "\n",
        "print(\"Train Loader Information:\")\n",
        "print(f\"  Number of batches: {len(train_loader)}\")\n",
        "print(f\"  Batch size: {train_loader.batch_size}\")\n",
        "# Get the dimension of a single batch\n",
        "for images, labels in train_loader:\n",
        "  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n",
        "  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n",
        "  break  # Exit the loop after processing one batch\n",
        "print()\n",
        "\n",
        "print(\"\\nValidation Loader Information:\")\n",
        "print(f\"  Number of batches: {len(val_loader)}\")\n",
        "print(f\"  Batch size: {val_loader.batch_size}\")\n",
        "# Get the dimension of a single batch\n",
        "for images, labels in val_loader:\n",
        "  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n",
        "  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n",
        "  break  # Exit the loop after processing one batch\n",
        "print()\n",
        "\n",
        "# Check for CUDA availability\n",
        "print(\"CUDA AVAIABILITY:\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
        "    print(\"Current GPU:\", torch.cuda.current_device())\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "# Print model architecture summary\n",
        "print(\"\\nMODEL ARCHITECTURE:\")\n",
        "print(model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}