{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# IMPORT\n",
        "import numpy as np\n",
        "import wandb\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import shutil\n",
        "import os                              # Import the 'os' module for changing directories\n",
        "os.chdir('/content/drive/MyDrive/FL')  # Change the directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0LweG_pBSd8",
        "outputId": "8f9d572c-7c9d-4d25-9c52-1434b6cfa8a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "from torch.utils.data import Subset, DataLoader, random_split\n",
        "\n",
        "from FederatedLearningProject.data.cifar100_loader import get_cifar100, create_iid_splits, create_non_iid_splits\n",
        "import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n",
        "from FederatedLearningProject.training.FedMETA import aggregate_with_task_arithmetic, aggregate_masks, distribution_function, train_server\n",
        "from FederatedLearningProject.training.model_editing import plot_all_layers_mask_sparsity\n",
        "\n",
        "from FederatedLearningProject.experiments import models\n",
        "import copy"
      ],
      "metadata": {
        "id": "rdG45zfHBdfd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "# Importa i moduli del tuo progetto\n",
        "from FederatedLearningProject.data import cifar100_loader\n",
        "from FederatedLearningProject import checkpoints\n",
        "from FederatedLearningProject.training import FedMETA, model_editing\n",
        "from FederatedLearningProject import experiments\n",
        "\n",
        "# Ricarica solo i moduli custom (NO torch)\n",
        "importlib.reload(cifar100_loader)\n",
        "importlib.reload(checkpoints.checkpointing)\n",
        "importlib.reload(FedMETA)\n",
        "importlib.reload(model_editing)\n",
        "importlib.reload(experiments.models)\n",
        "\n",
        "# Re-bind: importa di nuovo funzioni/classi/alias che usi nel codice\n",
        "from FederatedLearningProject.data.cifar100_loader import (\n",
        "    get_cifar100, create_iid_splits, create_non_iid_splits\n",
        ")\n",
        "\n",
        "import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n",
        "\n",
        "from FederatedLearningProject.training.FedMETA import (\n",
        "    aggregate_with_task_arithmetic,\n",
        "    aggregate_masks,\n",
        "    distribution_function,\n",
        "    train_server\n",
        ")\n",
        "\n",
        "from FederatedLearningProject.training.model_editing import (\n",
        "    plot_all_layers_mask_sparsity\n",
        ")\n",
        "\n",
        "from FederatedLearningProject.experiments import models\n"
      ],
      "metadata": {
        "id": "CHeYsngVyf8z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login() # Ask for your APIw key for logging in to the wandb library."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "om6P1kf38H1F",
        "outputId": "91e8e05c-1239-4beb-8d0c-deebc104503d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdepetrofabio\u001b[0m (\u001b[33mdepetrofabio-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_name = \"dino_vits16_J4\"\n",
        "project_name = \"FederatedProjectPROVA_PARTE4\"\n",
        "\n",
        "# Generate a unique run name for each iteration\n",
        "run_name = f\"{model_name}_rounds_prova\"\n",
        "# INITIALIZE W&B for each new run\n",
        "wandb.init(\n",
        "    project=project_name,\n",
        "    name=run_name,\n",
        "    config={\n",
        "        \"model\": model_name,\n",
        "        \"num_rounds\": 100, # Use the current num_rounds_val\n",
        "        \"batch_size\": 128, # Using test_loader's batch_size as a placeholder\n",
        "    },\n",
        "    reinit=True # Important: Allows re-initialization of wandb in a loop\n",
        ")\n",
        "\n",
        "# Copy your config\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "5q_imKpr8maZ",
        "outputId": "1aebf2fd-f871-448a-84c5-abf53b831788"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to True."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dino_vits16_J4_rounds_prova</strong> at: <a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProject/runs/6uingb69' target=\"_blank\">https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProject/runs/6uingb69</a><br> View project at: <a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProject' target=\"_blank\">https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProject</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250704_193940-6uingb69/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/FL/wandb/run-20250704_193950-0tmx920a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4/runs/0tmx920a' target=\"_blank\">dino_vits16_J4_rounds_prova</a></strong> to <a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4' target=\"_blank\">https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4/runs/0tmx920a' target=\"_blank\">https://wandb.ai/depetrofabio-politecnico-di-torino/FederatedProjectPROVA_PARTE4/runs/0tmx920a</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print the content of the folder FederatedLearningProject.data.masks\n",
        "print(os.listdir('FederatedLearningProject/masks'))\n",
        "\n",
        "val_set = torch.load('FederatedLearningProject/masks/val_set.pth', weights_only=False)\n",
        "train_set = torch.load('FederatedLearningProject/masks/train_set.pth', weights_only=False)\n",
        "test_set = torch.load('FederatedLearningProject/masks/test_set.pth', weights_only=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNXWWiquDGXR",
        "outputId": "3412a1c7-3f73-4dff-d28e-4161a788e244"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['train_set.pth', 'val_set.pth', 'test_set.pth', 'client_masks_iid.pth', 'client_masks_non_iid_1.pth']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "o_model = models.LinearFlexibleDino(num_layers_to_freeze=12)\n",
        "local_masks = torch.load('FederatedLearningProject/masks/client_masks_non_iid_1.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF_H3hYjMbZJ",
        "outputId": "00740da4-6831-4258-c00c-3839991d2e67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = copy.deepcopy(o_model)\n",
        "model_checkpoint = torch.load(\"FederatedLearningProject/checkpoints/FL_NON_IID(1)/dino_vits_16_non_iid(1)_local_steps_4_checkpoint.pth\")"
      ],
      "metadata": {
        "id": "jE-qROlN-dUQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(model_checkpoint['model_state_dict'])\n",
        "model.debug()"
      ],
      "metadata": {
        "id": "_uHOtcok-wlS",
        "outputId": "27baef68-29c0-45b0-91a5-3984792c7bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Model ---\n",
            "Model is primarily on device: cuda:0\n",
            "Model overall mode: Train\n",
            "\n",
            "Parameter Details (Name | Device | Requires Grad? | Inferred Block | Module Mode):\n",
            "- backbone.cls_token                                 | cuda:0     | False           | N/A             | Train\n",
            "- backbone.pos_embed                                 | cuda:0     | False           | N/A             | Train\n",
            "- backbone.patch_embed.proj.weight                   | cuda:0     | False           | N/A             | Train\n",
            "- backbone.patch_embed.proj.bias                     | cuda:0     | False           | N/A             | Train\n",
            "- backbone.blocks.0.norm1.weight                     | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.norm1.bias                       | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.attn.qkv.weight                  | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.attn.qkv.bias                    | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.attn.proj.weight                 | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.attn.proj.bias                   | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.norm2.weight                     | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.norm2.bias                       | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.mlp.fc1.weight                   | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.mlp.fc1.bias                     | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.mlp.fc2.weight                   | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.0.mlp.fc2.bias                     | cuda:0     | False           | Block 0         | Eval\n",
            "- backbone.blocks.1.norm1.weight                     | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.norm1.bias                       | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.attn.qkv.weight                  | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.attn.qkv.bias                    | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.attn.proj.weight                 | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.attn.proj.bias                   | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.norm2.weight                     | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.norm2.bias                       | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.mlp.fc1.weight                   | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.mlp.fc1.bias                     | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.mlp.fc2.weight                   | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.1.mlp.fc2.bias                     | cuda:0     | False           | Block 1         | Eval\n",
            "- backbone.blocks.2.norm1.weight                     | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.norm1.bias                       | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.attn.qkv.weight                  | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.attn.qkv.bias                    | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.attn.proj.weight                 | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.attn.proj.bias                   | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.norm2.weight                     | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.norm2.bias                       | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.mlp.fc1.weight                   | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.mlp.fc1.bias                     | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.mlp.fc2.weight                   | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.2.mlp.fc2.bias                     | cuda:0     | False           | Block 2         | Eval\n",
            "- backbone.blocks.3.norm1.weight                     | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.norm1.bias                       | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.attn.qkv.weight                  | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.attn.qkv.bias                    | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.attn.proj.weight                 | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.attn.proj.bias                   | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.norm2.weight                     | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.norm2.bias                       | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.mlp.fc1.weight                   | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.mlp.fc1.bias                     | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.mlp.fc2.weight                   | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.3.mlp.fc2.bias                     | cuda:0     | False           | Block 3         | Eval\n",
            "- backbone.blocks.4.norm1.weight                     | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.norm1.bias                       | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.attn.qkv.weight                  | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.attn.qkv.bias                    | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.attn.proj.weight                 | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.attn.proj.bias                   | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.norm2.weight                     | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.norm2.bias                       | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.mlp.fc1.weight                   | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.mlp.fc1.bias                     | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.mlp.fc2.weight                   | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.4.mlp.fc2.bias                     | cuda:0     | False           | Block 4         | Eval\n",
            "- backbone.blocks.5.norm1.weight                     | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.norm1.bias                       | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.attn.qkv.weight                  | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.attn.qkv.bias                    | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.attn.proj.weight                 | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.attn.proj.bias                   | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.norm2.weight                     | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.norm2.bias                       | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.mlp.fc1.weight                   | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.mlp.fc1.bias                     | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.mlp.fc2.weight                   | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.5.mlp.fc2.bias                     | cuda:0     | False           | Block 5         | Eval\n",
            "- backbone.blocks.6.norm1.weight                     | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.norm1.bias                       | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.attn.qkv.weight                  | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.attn.qkv.bias                    | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.attn.proj.weight                 | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.attn.proj.bias                   | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.norm2.weight                     | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.norm2.bias                       | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.mlp.fc1.weight                   | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.mlp.fc1.bias                     | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.mlp.fc2.weight                   | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.6.mlp.fc2.bias                     | cuda:0     | False           | Block 6         | Eval\n",
            "- backbone.blocks.7.norm1.weight                     | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.norm1.bias                       | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.attn.qkv.weight                  | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.attn.qkv.bias                    | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.attn.proj.weight                 | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.attn.proj.bias                   | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.norm2.weight                     | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.norm2.bias                       | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.mlp.fc1.weight                   | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.mlp.fc1.bias                     | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.mlp.fc2.weight                   | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.7.mlp.fc2.bias                     | cuda:0     | False           | Block 7         | Eval\n",
            "- backbone.blocks.8.norm1.weight                     | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.norm1.bias                       | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.attn.qkv.weight                  | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.attn.qkv.bias                    | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.attn.proj.weight                 | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.attn.proj.bias                   | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.norm2.weight                     | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.norm2.bias                       | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.mlp.fc1.weight                   | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.mlp.fc1.bias                     | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.mlp.fc2.weight                   | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.8.mlp.fc2.bias                     | cuda:0     | False           | Block 8         | Eval\n",
            "- backbone.blocks.9.norm1.weight                     | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.norm1.bias                       | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.attn.qkv.weight                  | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.attn.qkv.bias                    | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.attn.proj.weight                 | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.attn.proj.bias                   | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.norm2.weight                     | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.norm2.bias                       | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.mlp.fc1.weight                   | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.mlp.fc1.bias                     | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.mlp.fc2.weight                   | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.9.mlp.fc2.bias                     | cuda:0     | False           | Block 9         | Eval\n",
            "- backbone.blocks.10.norm1.weight                    | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.norm1.bias                      | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.attn.qkv.weight                 | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.attn.qkv.bias                   | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.attn.proj.weight                | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.attn.proj.bias                  | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.norm2.weight                    | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.norm2.bias                      | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.mlp.fc1.weight                  | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.mlp.fc1.bias                    | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.mlp.fc2.weight                  | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.10.mlp.fc2.bias                    | cuda:0     | False           | Block 10        | Eval\n",
            "- backbone.blocks.11.norm1.weight                    | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.norm1.bias                      | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.attn.qkv.weight                 | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.attn.qkv.bias                   | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.attn.proj.weight                | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.attn.proj.bias                  | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.norm2.weight                    | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.norm2.bias                      | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.mlp.fc1.weight                  | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.mlp.fc1.bias                    | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.mlp.fc2.weight                  | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.blocks.11.mlp.fc2.bias                    | cuda:0     | False           | Block 11        | Eval\n",
            "- backbone.norm.weight                               | cuda:0     | True            | N/A             | Train\n",
            "- backbone.norm.bias                                 | cuda:0     | True            | N/A             | Train\n",
            "- head.weight                                        | cuda:0     | True            | N/A             | Train\n",
            "- head.bias                                          | cuda:0     | True            | N/A             | Train\n",
            "--- End Debugging Model ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.unfreeze(12)\n",
        "model.debug()"
      ],
      "metadata": {
        "id": "gr_aqMEW-zwW",
        "outputId": "79018832-2ed9-4764-a9d5-51dfc3147ca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Debugging Model ---\n",
            "Model is primarily on device: cuda:0\n",
            "Model overall mode: Train\n",
            "\n",
            "Parameter Details (Name | Device | Requires Grad? | Inferred Block | Module Mode):\n",
            "- backbone.cls_token                                 | cuda:0     | True            | N/A             | Train\n",
            "- backbone.pos_embed                                 | cuda:0     | True            | N/A             | Train\n",
            "- backbone.patch_embed.proj.weight                   | cuda:0     | True            | N/A             | Train\n",
            "- backbone.patch_embed.proj.bias                     | cuda:0     | True            | N/A             | Train\n",
            "- backbone.blocks.0.norm1.weight                     | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.norm1.bias                       | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.attn.qkv.weight                  | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.attn.qkv.bias                    | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.attn.proj.weight                 | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.attn.proj.bias                   | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.norm2.weight                     | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.norm2.bias                       | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.mlp.fc1.weight                   | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.mlp.fc1.bias                     | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.mlp.fc2.weight                   | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.0.mlp.fc2.bias                     | cuda:0     | True            | Block 0         | Train\n",
            "- backbone.blocks.1.norm1.weight                     | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.norm1.bias                       | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.attn.qkv.weight                  | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.attn.qkv.bias                    | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.attn.proj.weight                 | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.attn.proj.bias                   | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.norm2.weight                     | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.norm2.bias                       | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.mlp.fc1.weight                   | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.mlp.fc1.bias                     | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.mlp.fc2.weight                   | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.1.mlp.fc2.bias                     | cuda:0     | True            | Block 1         | Train\n",
            "- backbone.blocks.2.norm1.weight                     | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.norm1.bias                       | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.attn.qkv.weight                  | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.attn.qkv.bias                    | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.attn.proj.weight                 | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.attn.proj.bias                   | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.norm2.weight                     | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.norm2.bias                       | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.mlp.fc1.weight                   | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.mlp.fc1.bias                     | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.mlp.fc2.weight                   | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.2.mlp.fc2.bias                     | cuda:0     | True            | Block 2         | Train\n",
            "- backbone.blocks.3.norm1.weight                     | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.norm1.bias                       | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.attn.qkv.weight                  | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.attn.qkv.bias                    | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.attn.proj.weight                 | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.attn.proj.bias                   | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.norm2.weight                     | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.norm2.bias                       | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.mlp.fc1.weight                   | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.mlp.fc1.bias                     | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.mlp.fc2.weight                   | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.3.mlp.fc2.bias                     | cuda:0     | True            | Block 3         | Train\n",
            "- backbone.blocks.4.norm1.weight                     | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.norm1.bias                       | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.attn.qkv.weight                  | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.attn.qkv.bias                    | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.attn.proj.weight                 | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.attn.proj.bias                   | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.norm2.weight                     | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.norm2.bias                       | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.mlp.fc1.weight                   | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.mlp.fc1.bias                     | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.mlp.fc2.weight                   | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.4.mlp.fc2.bias                     | cuda:0     | True            | Block 4         | Train\n",
            "- backbone.blocks.5.norm1.weight                     | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.norm1.bias                       | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.attn.qkv.weight                  | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.attn.qkv.bias                    | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.attn.proj.weight                 | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.attn.proj.bias                   | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.norm2.weight                     | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.norm2.bias                       | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.mlp.fc1.weight                   | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.mlp.fc1.bias                     | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.mlp.fc2.weight                   | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.5.mlp.fc2.bias                     | cuda:0     | True            | Block 5         | Train\n",
            "- backbone.blocks.6.norm1.weight                     | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.norm1.bias                       | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.attn.qkv.weight                  | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.attn.qkv.bias                    | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.attn.proj.weight                 | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.attn.proj.bias                   | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.norm2.weight                     | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.norm2.bias                       | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.mlp.fc1.weight                   | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.mlp.fc1.bias                     | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.mlp.fc2.weight                   | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.6.mlp.fc2.bias                     | cuda:0     | True            | Block 6         | Train\n",
            "- backbone.blocks.7.norm1.weight                     | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.norm1.bias                       | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.attn.qkv.weight                  | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.attn.qkv.bias                    | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.attn.proj.weight                 | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.attn.proj.bias                   | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.norm2.weight                     | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.norm2.bias                       | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.mlp.fc1.weight                   | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.mlp.fc1.bias                     | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.mlp.fc2.weight                   | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.7.mlp.fc2.bias                     | cuda:0     | True            | Block 7         | Train\n",
            "- backbone.blocks.8.norm1.weight                     | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.norm1.bias                       | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.attn.qkv.weight                  | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.attn.qkv.bias                    | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.attn.proj.weight                 | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.attn.proj.bias                   | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.norm2.weight                     | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.norm2.bias                       | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.mlp.fc1.weight                   | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.mlp.fc1.bias                     | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.mlp.fc2.weight                   | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.8.mlp.fc2.bias                     | cuda:0     | True            | Block 8         | Train\n",
            "- backbone.blocks.9.norm1.weight                     | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.norm1.bias                       | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.attn.qkv.weight                  | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.attn.qkv.bias                    | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.attn.proj.weight                 | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.attn.proj.bias                   | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.norm2.weight                     | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.norm2.bias                       | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.mlp.fc1.weight                   | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.mlp.fc1.bias                     | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.mlp.fc2.weight                   | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.9.mlp.fc2.bias                     | cuda:0     | True            | Block 9         | Train\n",
            "- backbone.blocks.10.norm1.weight                    | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.norm1.bias                      | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.attn.qkv.weight                 | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.attn.qkv.bias                   | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.attn.proj.weight                | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.attn.proj.bias                  | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.norm2.weight                    | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.norm2.bias                      | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.mlp.fc1.weight                  | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.mlp.fc1.bias                    | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.mlp.fc2.weight                  | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.10.mlp.fc2.bias                    | cuda:0     | True            | Block 10        | Train\n",
            "- backbone.blocks.11.norm1.weight                    | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.norm1.bias                      | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.attn.qkv.weight                 | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.attn.qkv.bias                   | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.attn.proj.weight                | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.attn.proj.bias                  | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.norm2.weight                    | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.norm2.bias                      | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.mlp.fc1.weight                  | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.mlp.fc1.bias                    | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.mlp.fc2.weight                  | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.blocks.11.mlp.fc2.bias                    | cuda:0     | True            | Block 11        | Train\n",
            "- backbone.norm.weight                               | cuda:0     | True            | N/A             | Train\n",
            "- backbone.norm.bias                                 | cuda:0     | True            | N/A             | Train\n",
            "- head.weight                                        | cuda:0     | True            | N/A             | Train\n",
            "- head.bias                                          | cuda:0     | True            | N/A             | Train\n",
            "--- End Debugging Model ---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_mask = aggregate_masks(local_masks)"
      ],
      "metadata": {
        "id": "dBH17S-OuX8E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partition_masks = distribution_function(final_mask, number_clients=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEkS1vyewrYn",
        "outputId": "595d2957-d3f5-4968-a059-5c12f6e92d6e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 21293568\n",
            "Masked parameters (zeros): 19902014\n",
            "Unmasked parameters (ones): 1391554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client_dataset = create_non_iid_splits(train_set, num_clients=100, classes_per_client=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEd2WQIgvduj",
        "outputId": "47069211-26f2-407c-a32b-d469aadd5f37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has 40000 samples across 100 classes.\n",
            "Creating 100 non IID splits with 1 classes each.\n",
            "\n",
            "\n",
            "Each of the 100 classes split into 1 shards.\n",
            "\n",
            "Checking unique classes that each client sees:\n",
            "Client 0 has samples from classes: {np.int64(0)}\n",
            "Total: 1\n",
            "Client 1 has samples from classes: {np.int64(1)}\n",
            "Total: 1\n",
            "Client 2 has samples from classes: {np.int64(2)}\n",
            "Total: 1\n",
            "Client 3 has samples from classes: {np.int64(3)}\n",
            "Total: 1\n",
            "Client 4 has samples from classes: {np.int64(4)}\n",
            "Total: 1\n",
            "Client 5 has samples from classes: {np.int64(5)}\n",
            "Total: 1\n",
            "Client 6 has samples from classes: {np.int64(6)}\n",
            "Total: 1\n",
            "Client 7 has samples from classes: {np.int64(7)}\n",
            "Total: 1\n",
            "Client 8 has samples from classes: {np.int64(8)}\n",
            "Total: 1\n",
            "Client 9 has samples from classes: {np.int64(9)}\n",
            "Total: 1\n",
            "Client 10 has samples from classes: {np.int64(10)}\n",
            "Total: 1\n",
            "Client 11 has samples from classes: {np.int64(11)}\n",
            "Total: 1\n",
            "Client 12 has samples from classes: {np.int64(12)}\n",
            "Total: 1\n",
            "Client 13 has samples from classes: {np.int64(13)}\n",
            "Total: 1\n",
            "Client 14 has samples from classes: {np.int64(14)}\n",
            "Total: 1\n",
            "Client 15 has samples from classes: {np.int64(15)}\n",
            "Total: 1\n",
            "Client 16 has samples from classes: {np.int64(16)}\n",
            "Total: 1\n",
            "Client 17 has samples from classes: {np.int64(17)}\n",
            "Total: 1\n",
            "Client 18 has samples from classes: {np.int64(18)}\n",
            "Total: 1\n",
            "Client 19 has samples from classes: {np.int64(19)}\n",
            "Total: 1\n",
            "Client 20 has samples from classes: {np.int64(20)}\n",
            "Total: 1\n",
            "Client 21 has samples from classes: {np.int64(21)}\n",
            "Total: 1\n",
            "Client 22 has samples from classes: {np.int64(22)}\n",
            "Total: 1\n",
            "Client 23 has samples from classes: {np.int64(23)}\n",
            "Total: 1\n",
            "Client 24 has samples from classes: {np.int64(24)}\n",
            "Total: 1\n",
            "Client 25 has samples from classes: {np.int64(25)}\n",
            "Total: 1\n",
            "Client 26 has samples from classes: {np.int64(26)}\n",
            "Total: 1\n",
            "Client 27 has samples from classes: {np.int64(27)}\n",
            "Total: 1\n",
            "Client 28 has samples from classes: {np.int64(28)}\n",
            "Total: 1\n",
            "Client 29 has samples from classes: {np.int64(29)}\n",
            "Total: 1\n",
            "Client 30 has samples from classes: {np.int64(30)}\n",
            "Total: 1\n",
            "Client 31 has samples from classes: {np.int64(31)}\n",
            "Total: 1\n",
            "Client 32 has samples from classes: {np.int64(32)}\n",
            "Total: 1\n",
            "Client 33 has samples from classes: {np.int64(33)}\n",
            "Total: 1\n",
            "Client 34 has samples from classes: {np.int64(34)}\n",
            "Total: 1\n",
            "Client 35 has samples from classes: {np.int64(35)}\n",
            "Total: 1\n",
            "Client 36 has samples from classes: {np.int64(36)}\n",
            "Total: 1\n",
            "Client 37 has samples from classes: {np.int64(37)}\n",
            "Total: 1\n",
            "Client 38 has samples from classes: {np.int64(38)}\n",
            "Total: 1\n",
            "Client 39 has samples from classes: {np.int64(39)}\n",
            "Total: 1\n",
            "Client 40 has samples from classes: {np.int64(40)}\n",
            "Total: 1\n",
            "Client 41 has samples from classes: {np.int64(41)}\n",
            "Total: 1\n",
            "Client 42 has samples from classes: {np.int64(42)}\n",
            "Total: 1\n",
            "Client 43 has samples from classes: {np.int64(43)}\n",
            "Total: 1\n",
            "Client 44 has samples from classes: {np.int64(44)}\n",
            "Total: 1\n",
            "Client 45 has samples from classes: {np.int64(45)}\n",
            "Total: 1\n",
            "Client 46 has samples from classes: {np.int64(46)}\n",
            "Total: 1\n",
            "Client 47 has samples from classes: {np.int64(47)}\n",
            "Total: 1\n",
            "Client 48 has samples from classes: {np.int64(48)}\n",
            "Total: 1\n",
            "Client 49 has samples from classes: {np.int64(49)}\n",
            "Total: 1\n",
            "Client 50 has samples from classes: {np.int64(50)}\n",
            "Total: 1\n",
            "Client 51 has samples from classes: {np.int64(51)}\n",
            "Total: 1\n",
            "Client 52 has samples from classes: {np.int64(52)}\n",
            "Total: 1\n",
            "Client 53 has samples from classes: {np.int64(53)}\n",
            "Total: 1\n",
            "Client 54 has samples from classes: {np.int64(54)}\n",
            "Total: 1\n",
            "Client 55 has samples from classes: {np.int64(55)}\n",
            "Total: 1\n",
            "Client 56 has samples from classes: {np.int64(56)}\n",
            "Total: 1\n",
            "Client 57 has samples from classes: {np.int64(57)}\n",
            "Total: 1\n",
            "Client 58 has samples from classes: {np.int64(58)}\n",
            "Total: 1\n",
            "Client 59 has samples from classes: {np.int64(59)}\n",
            "Total: 1\n",
            "Client 60 has samples from classes: {np.int64(60)}\n",
            "Total: 1\n",
            "Client 61 has samples from classes: {np.int64(61)}\n",
            "Total: 1\n",
            "Client 62 has samples from classes: {np.int64(62)}\n",
            "Total: 1\n",
            "Client 63 has samples from classes: {np.int64(63)}\n",
            "Total: 1\n",
            "Client 64 has samples from classes: {np.int64(64)}\n",
            "Total: 1\n",
            "Client 65 has samples from classes: {np.int64(65)}\n",
            "Total: 1\n",
            "Client 66 has samples from classes: {np.int64(66)}\n",
            "Total: 1\n",
            "Client 67 has samples from classes: {np.int64(67)}\n",
            "Total: 1\n",
            "Client 68 has samples from classes: {np.int64(68)}\n",
            "Total: 1\n",
            "Client 69 has samples from classes: {np.int64(69)}\n",
            "Total: 1\n",
            "Client 70 has samples from classes: {np.int64(70)}\n",
            "Total: 1\n",
            "Client 71 has samples from classes: {np.int64(71)}\n",
            "Total: 1\n",
            "Client 72 has samples from classes: {np.int64(72)}\n",
            "Total: 1\n",
            "Client 73 has samples from classes: {np.int64(73)}\n",
            "Total: 1\n",
            "Client 74 has samples from classes: {np.int64(74)}\n",
            "Total: 1\n",
            "Client 75 has samples from classes: {np.int64(75)}\n",
            "Total: 1\n",
            "Client 76 has samples from classes: {np.int64(76)}\n",
            "Total: 1\n",
            "Client 77 has samples from classes: {np.int64(77)}\n",
            "Total: 1\n",
            "Client 78 has samples from classes: {np.int64(78)}\n",
            "Total: 1\n",
            "Client 79 has samples from classes: {np.int64(79)}\n",
            "Total: 1\n",
            "Client 80 has samples from classes: {np.int64(80)}\n",
            "Total: 1\n",
            "Client 81 has samples from classes: {np.int64(81)}\n",
            "Total: 1\n",
            "Client 82 has samples from classes: {np.int64(82)}\n",
            "Total: 1\n",
            "Client 83 has samples from classes: {np.int64(83)}\n",
            "Total: 1\n",
            "Client 84 has samples from classes: {np.int64(84)}\n",
            "Total: 1\n",
            "Client 85 has samples from classes: {np.int64(85)}\n",
            "Total: 1\n",
            "Client 86 has samples from classes: {np.int64(86)}\n",
            "Total: 1\n",
            "Client 87 has samples from classes: {np.int64(87)}\n",
            "Total: 1\n",
            "Client 88 has samples from classes: {np.int64(88)}\n",
            "Total: 1\n",
            "Client 89 has samples from classes: {np.int64(89)}\n",
            "Total: 1\n",
            "Client 90 has samples from classes: {np.int64(90)}\n",
            "Total: 1\n",
            "Client 91 has samples from classes: {np.int64(91)}\n",
            "Total: 1\n",
            "Client 92 has samples from classes: {np.int64(92)}\n",
            "Total: 1\n",
            "Client 93 has samples from classes: {np.int64(93)}\n",
            "Total: 1\n",
            "Client 94 has samples from classes: {np.int64(94)}\n",
            "Total: 1\n",
            "Client 95 has samples from classes: {np.int64(95)}\n",
            "Total: 1\n",
            "Client 96 has samples from classes: {np.int64(96)}\n",
            "Total: 1\n",
            "Client 97 has samples from classes: {np.int64(97)}\n",
            "Total: 1\n",
            "Client 98 has samples from classes: {np.int64(98)}\n",
            "Total: 1\n",
            "Client 99 has samples from classes: {np.int64(99)}\n",
            "Total: 1\n",
            "\n",
            "\n",
            "Client 0: 383 samples\n",
            "Client 1: 390 samples\n",
            "Client 2: 391 samples\n",
            "Client 3: 407 samples\n",
            "Client 4: 404 samples\n",
            "Client 5: 402 samples\n",
            "Client 6: 390 samples\n",
            "Client 7: 409 samples\n",
            "Client 8: 420 samples\n",
            "Client 9: 397 samples\n",
            "Client 10: 397 samples\n",
            "Client 11: 402 samples\n",
            "Client 12: 407 samples\n",
            "Client 13: 401 samples\n",
            "Client 14: 382 samples\n",
            "Client 15: 400 samples\n",
            "Client 16: 406 samples\n",
            "Client 17: 407 samples\n",
            "Client 18: 403 samples\n",
            "Client 19: 405 samples\n",
            "Client 20: 392 samples\n",
            "Client 21: 399 samples\n",
            "Client 22: 387 samples\n",
            "Client 23: 403 samples\n",
            "Client 24: 386 samples\n",
            "Client 25: 396 samples\n",
            "Client 26: 395 samples\n",
            "Client 27: 404 samples\n",
            "Client 28: 397 samples\n",
            "Client 29: 406 samples\n",
            "Client 30: 411 samples\n",
            "Client 31: 412 samples\n",
            "Client 32: 408 samples\n",
            "Client 33: 394 samples\n",
            "Client 34: 399 samples\n",
            "Client 35: 405 samples\n",
            "Client 36: 404 samples\n",
            "Client 37: 409 samples\n",
            "Client 38: 398 samples\n",
            "Client 39: 401 samples\n",
            "Client 40: 400 samples\n",
            "Client 41: 402 samples\n",
            "Client 42: 416 samples\n",
            "Client 43: 399 samples\n",
            "Client 44: 406 samples\n",
            "Client 45: 409 samples\n",
            "Client 46: 394 samples\n",
            "Client 47: 404 samples\n",
            "Client 48: 418 samples\n",
            "Client 49: 414 samples\n",
            "Client 50: 391 samples\n",
            "Client 51: 401 samples\n",
            "Client 52: 408 samples\n",
            "Client 53: 377 samples\n",
            "Client 54: 390 samples\n",
            "Client 55: 409 samples\n",
            "Client 56: 398 samples\n",
            "Client 57: 380 samples\n",
            "Client 58: 400 samples\n",
            "Client 59: 407 samples\n",
            "Client 60: 390 samples\n",
            "Client 61: 397 samples\n",
            "Client 62: 411 samples\n",
            "Client 63: 395 samples\n",
            "Client 64: 416 samples\n",
            "Client 65: 401 samples\n",
            "Client 66: 400 samples\n",
            "Client 67: 396 samples\n",
            "Client 68: 406 samples\n",
            "Client 69: 398 samples\n",
            "Client 70: 381 samples\n",
            "Client 71: 392 samples\n",
            "Client 72: 415 samples\n",
            "Client 73: 398 samples\n",
            "Client 74: 399 samples\n",
            "Client 75: 408 samples\n",
            "Client 76: 379 samples\n",
            "Client 77: 408 samples\n",
            "Client 78: 397 samples\n",
            "Client 79: 412 samples\n",
            "Client 80: 396 samples\n",
            "Client 81: 404 samples\n",
            "Client 82: 409 samples\n",
            "Client 83: 394 samples\n",
            "Client 84: 408 samples\n",
            "Client 85: 409 samples\n",
            "Client 86: 400 samples\n",
            "Client 87: 395 samples\n",
            "Client 88: 410 samples\n",
            "Client 89: 393 samples\n",
            "Client 90: 409 samples\n",
            "Client 91: 405 samples\n",
            "Client 92: 393 samples\n",
            "Client 93: 387 samples\n",
            "Client 94: 395 samples\n",
            "Client 95: 397 samples\n",
            "Client 96: 385 samples\n",
            "Client 97: 406 samples\n",
            "Client 98: 385 samples\n",
            "Client 99: 389 samples\n",
            "Client partitions created from <FederatedLearningProject.data.cifar100_loader.TransformedSubset object at 0x79eee8640050>, passed as an argument to the function. All the transformations were mantained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_config = {\n",
        "    'lr': 0.01,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 0.0001\n",
        "}\n",
        "\n",
        "model.to_cuda()\n",
        "\n",
        "checkpoint_path = 'FederatedLearningProject/checkpoints/'\n",
        "val_loader = DataLoader(val_set, batch_size=128, shuffle=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_server(model, num_rounds=100, client_dataset=client_dataset, client_masks=partition_masks, optimizer_config=optimizer_config, device='cuda', frac=0.1, batch_size=128, val_loader=val_loader, checkpoint_path=checkpoint_path, criterion=criterion)"
      ],
      "metadata": {
        "id": "yh7p3OsfuoNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "outputId": "c5154c57-70a1-48fd-de3c-3102bf9a35ee"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moving model to cuda\n",
            "\n",
            "Round 5/100\n",
            "Selected Clients: [71 90 80 72  5  1 45 95 85 39]\n",
            "Avg Client Loss: 5.2103 | Avg Client Accuracy: 22.59%\n",
            "Evaluation Loss: 4.4985 | Val Accuracy: 32.76%\n",
            "--------------------------------------------------\n",
            "\n",
            "Round 10/100\n",
            "Selected Clients: [ 3 97 27 89  5 45  7 15 36 46]\n",
            "Avg Client Loss: 7.4155 | Avg Client Accuracy: 9.65%\n",
            "Evaluation Loss: 4.4985 | Val Accuracy: 32.76%\n",
            "--------------------------------------------------\n",
            "\n",
            "Round 15/100\n",
            "Selected Clients: [74 72 24 43 88 82 97  7 92 19]\n",
            "Avg Client Loss: 5.7687 | Avg Client Accuracy: 28.48%\n",
            "Evaluation Loss: 4.4985 | Val Accuracy: 32.76%\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-4198751941.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/FL/FederatedLearningProject/training/FedMETA.py\u001b[0m in \u001b[0;36mtrain_server\u001b[0;34m(model, num_rounds, client_dataset, client_masks, optimizer_config, device, val_loader, checkpoint_path, n_rounds_log, num_clients, num_client_steps, frac, criterion, batch_size, debug, model_name)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mclient_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             delta, client_loss, client_accuracy, = train_client(\n\u001b[0m\u001b[1;32m    189\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mclient_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/FL/FederatedLearningProject/training/FedMETA.py\u001b[0m in \u001b[0;36mtrain_client\u001b[0;34m(model, client_loader, optimizer_config, client_mask, criterion, device, batch_size, num_local_steps)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mlocal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     old_weights = {\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/FL/FederatedLearningProject/training/FedMETA.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     old_weights = {\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     }\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}