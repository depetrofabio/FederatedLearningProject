{"cells":[{"cell_type":"code","execution_count":1,"id":"228da2b7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10751,"status":"ok","timestamp":1745394574773,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"228da2b7","outputId":"a1e8289d-fe4f-4bb5-e608-dc412af6b1e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import torch\n","from torchvision.datasets import CIFAR100\n","from torch.utils.data import Subset, DataLoader, random_split\n","import numpy as np\n","import wandb\n","# import torchvision\n","from torchvision import transforms\n","import torch.optim as optim\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","import shutil\n","import os                              # Import the 'os' module for changing directories\n","os.chdir('/content/drive/MyDrive/FL')  # Change the directory\n","\n","import git_manager\n","import FederatedLearningProject.data.cifar100_loader as loader\n","import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n","\n"]},{"cell_type":"code","execution_count":2,"id":"usCBOVfNjyQI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1081,"status":"ok","timestamp":1745394767736,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"usCBOVfNjyQI","outputId":"de11e9d6-34dd-45f6-9eba-f2a6826f9fd7"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnicco-to\u001b[0m (\u001b[33mnicco-to-politecnico-di-torino\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["wandb.login() # Ask for your API key for logging in to the wandb library."]},{"cell_type":"code","execution_count":3,"id":"f1uwQJOSKHgl","metadata":{"id":"f1uwQJOSKHgl","executionInfo":{"status":"ok","timestamp":1745394779226,"user_tz":-120,"elapsed":2045,"user":{"displayName":"Niccolò","userId":"00690440481202077065"}}},"outputs":[],"source":["dataset = loader.get_cifar100()\n","train_set, val_set = loader.split_train_val(dataset)\n","\n","# Create DataLoader\n","# DataLoader is a class from PyTorch's torch.utils.data module. It helps iterate through your dataset during training.\n","# batch_size=128: This specifies that during training, the model will process data in batches of 128 samples at a time.\n","# shuffle=True: It randomly shuffles the data within the train_set before each epoch (a full pass through the dataset). Shuffling helps prevent the model from learning patterns based on the order of data, leading to better generalization.\n","train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n","val_loader = DataLoader(val_set, batch_size=128, shuffle=False)\n"]},{"cell_type":"code","execution_count":null,"id":"RvPTB3BVOM4r","metadata":{"id":"RvPTB3BVOM4r"},"outputs":[],"source":["## Useful only to train a small fragment of dataset!\n","\n","# subset_indices = np.random.choice(len(train_set), size=10000, replace=False)\n","# subset_train_dataset = Subset(train_set, subset_indices)\n","# train_set, val_set = loader.split_train_val(subset_train_dataset)\n","# train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n","# val_loader = DataLoader(val_set, batch_size=128, shuffle=False)\n"]},{"cell_type":"code","execution_count":4,"id":"lVyiuu-pLHAd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":612,"status":"ok","timestamp":1745394810463,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"lVyiuu-pLHAd","outputId":"3fdcaf25-3783-41e8-f17a-5716249d15fa"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n"]},{"output_type":"execute_result","data":{"text/plain":["VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (blocks): ModuleList(\n","    (0-11): 12 x Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","  (head): Identity()\n",")"]},"metadata":{},"execution_count":4}],"source":["# --- MODEL LOADING AND SETUP ---\n","\n","# Load the pre-trained DINO ViT-S/16 model from PyTorch Hub\n","model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n","\n","# Device selection: Use GPU if available, otherwise CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)  # Move the model to the selected device"]},{"cell_type":"code","execution_count":5,"id":"RqiWVe89NEJC","metadata":{"executionInfo":{"elapsed":106,"status":"ok","timestamp":1745394824418,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"RqiWVe89NEJC"},"outputs":[],"source":["# --- OPTIMIZER AND LOSS FUNCTION ---\n","# Define the optimizer (Adam is used best Federico found for now)\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","# Define the loss function (Cross-entropy for multi-class classification)\n","loss_fn = torch.nn.CrossEntropyLoss()\n"]},{"cell_type":"code","execution_count":6,"id":"w2WnTA_ue2sX","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"elapsed":1307,"status":"ok","timestamp":1745394828620,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"w2WnTA_ue2sX","outputId":"2bb43b46-64fe-404b-aa6a-084a9b4eefda"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.9"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/FL/wandb/run-20250423_075345-a8dr31o7</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject/runs/a8dr31o7' target=\"_blank\">dino_vits16_run</a></strong> to <a href='https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject' target=\"_blank\">https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject/runs/a8dr31o7' target=\"_blank\">https://wandb.ai/nicco-to-politecnico-di-torino/FederatedProject/runs/a8dr31o7</a>"]},"metadata":{}}],"source":["# wandb.init() prepares the tracking of hyperparameters/metrics for later recording performance using wandb.log()\n","\n","model_name = \"dino_vits16\"\n","project_name = \"FederatedProject\"\n","run_name = f\"{model_name}_run\"\n","\n","# INITIALIZE W&B\n","wandb.init(\n","    project=project_name,\n","    name=run_name,\n","    config={\n","        \"model\": model_name,\n","        \"epochs\": 20,\n","        \"batch_size\": train_loader.batch_size,\n","        \"learning_rate\": optimizer.param_groups[0]['lr'],\n","        \"architecture\": model.__class__.__name__,\n","})\n","\n","# Copy your config\n","config = wandb.config\n"]},{"cell_type":"code","execution_count":7,"id":"mo4lpz6c3C-F","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1745394856927,"user":{"displayName":"Niccolò","userId":"00690440481202077065"},"user_tz":-120},"id":"mo4lpz6c3C-F"},"outputs":[],"source":["#  PERCORSO CHECKPOINT\n","checkpoint_dir = \"/content/drive/MyDrive/FL/FederatedLearningProject/checkpoints\"\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_checkpoint.pth\")    # we predefine the name of the file inside the specified folder (dir)"]},{"cell_type":"code","execution_count":9,"id":"cBdBMnxzyusN","metadata":{"id":"cBdBMnxzyusN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745394997337,"user_tz":-120,"elapsed":11,"user":{"displayName":"Niccolò","userId":"00690440481202077065"}},"outputId":"06a3d7a9-2102-4385-cf0b-0694f427a544"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Nessun checkpoint trovato, inizio da epoca 1.\n","\n"]}],"source":["# RECOVER CHECKPOINT\n","epoch, model_data = checkpointing.load_checkpoint(model, optimizer, checkpoint_dir)\n","try:\n","  print()\n","  print(f\"The 'model_data' dictionary contains the following keys: {list(model_data.keys())}\")\n","except: None"]},{"cell_type":"code","source":["\n"],"metadata":{"id":"SW7nBi07q_km"},"id":"SW7nBi07q_km","execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Display some informations ##\n","\n","print(\"Model:\", model_name)\n","print(\"Train set size:\", len(train_set))\n","print(\"Validation set size:\", len(val_set))\n","print(\"Batch size:\", train_loader.batch_size)\n","print(\"Number of epochs:\", config.epochs)\n","print(\"DataLoader: \")\n","print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n","print(\"Architecture:\", model.__class__.__name__)\n","print(\"Device:\", device)\n","print(\"Optimizer:\", optimizer)\n","print(\"Loss function:\", loss_fn)\n","print(\"Checkpoint directory:\", checkpoint_dir)\n","print(\"Checkpoint path:\", checkpoint_path)\n","print(\"Current epoch:\", epoch)\n","print()\n","\n","print(\"Train Loader Information:\")\n","print(f\"  Number of batches: {len(train_loader)}\")\n","print(f\"  Batch size: {train_loader.batch_size}\")\n","# Get the dimension of a single batch\n","for images, labels in train_loader:\n","  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n","  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n","  break  # Exit the loop after processing one batch\n","print()\n","\n","print(\"\\nValidation Loader Information:\")\n","print(f\"  Number of batches: {len(val_loader)}\")\n","print(f\"  Batch size: {val_loader.batch_size}\")\n","# Get the dimension of a single batch\n","for images, labels in val_loader:\n","  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n","  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n","  break  # Exit the loop after processing one batch\n","print()\n","\n","# Check for CUDA availability\n","print(\"CUDA AVAIABILITY:\")\n","if torch.cuda.is_available():\n","    print(\"CUDA is available. Using GPU.\")\n","    print(\"Number of GPUs:\", torch.cuda.device_count())\n","    print(\"Current GPU:\", torch.cuda.current_device())\n","    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n","else:\n","    print(\"CUDA is not available. Using CPU.\")\n","\n","# Print model architecture summary\n","print(\"\\nMODEL ARCHITECTURE:\")\n","print(model)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCT7l21Lpl9L","executionInfo":{"status":"ok","timestamp":1745395528097,"user_tz":-120,"elapsed":152,"user":{"displayName":"Niccolò","userId":"00690440481202077065"}},"outputId":"0cb76b34-8f8e-4837-ab3e-15ee977ea76f"},"id":"lCT7l21Lpl9L","execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: dino_vits16\n","Train set size: 40000\n","Validation set size: 10000\n","Batch size: 128\n","Number of epochs: 20\n","DataLoader: \n","Learning rate: 0.0001\n","Architecture: VisionTransformer\n","Device: cuda\n","Optimizer: Adam (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    lr: 0.0001\n","    maximize: False\n","    weight_decay: 0\n",")\n","Loss function: CrossEntropyLoss()\n","Checkpoint directory: /content/drive/MyDrive/FL/FederatedLearningProject/checkpoints\n","Checkpoint path: /content/drive/MyDrive/FL/FederatedLearningProject/checkpoints/dino_vits16_checkpoint.pth\n","Current epoch: 1\n","\n","Train Loader Information:\n","  Number of batches: 313\n","  Batch size: 128\n","  Dimension of 1 batch (images): torch.Size([128, 3, 32, 32])\n","  Dimension of 1 batch (labels): torch.Size([128])\n","\n","\n","Validation Loader Information:\n","  Number of batches: 79\n","  Batch size: 128\n","  Dimension of 1 batch (images): torch.Size([128, 3, 32, 32])\n","  Dimension of 1 batch (labels): torch.Size([128])\n","\n","CUDA AVAIABILITY:\n","CUDA is available. Using GPU.\n","Number of GPUs: 1\n","Current GPU: 0\n","GPU Name: Tesla T4\n","\n","MODEL ARCHITECTURE:\n","VisionTransformer(\n","  (patch_embed): PatchEmbed(\n","    (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n","  )\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (blocks): ModuleList(\n","    (0-11): 12 x Block(\n","      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (attn): Attention(\n","        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n","        (attn_drop): Dropout(p=0.0, inplace=False)\n","        (proj): Linear(in_features=384, out_features=384, bias=True)\n","        (proj_drop): Dropout(p=0.0, inplace=False)\n","      )\n","      (drop_path): Identity()\n","      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","      (mlp): Mlp(\n","        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n","        (act): GELU(approximate='none')\n","        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n","        (drop): Dropout(p=0.0, inplace=False)\n","      )\n","    )\n","  )\n","  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n","  (head): Identity()\n",")\n"]}]},{"cell_type":"code","execution_count":null,"id":"xlonXgCbzymY","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"xlonXgCbzymY","executionInfo":{"status":"error","timestamp":1745393205562,"user_tz":-120,"elapsed":311853,"user":{"displayName":"Niccolò","userId":"00690440481202077065"}},"outputId":"6018f7eb-545d-4451-9cea-a8ba0ccae391"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1] Train Loss: 0.4402,  Train Accuracy: 88.17%, Val Loss: 0.4505, Val Accuracy: 87.97%\n","[Epoch 2] Train Loss: 0.2831,  Train Accuracy: 91.53%, Val Loss: 0.5059, Val Accuracy: 86.36%\n","[Epoch 3] Train Loss: 0.2367,  Train Accuracy: 92.57%, Val Loss: 0.5721, Val Accuracy: 85.18%\n","[Epoch 4] Train Loss: 0.2128,  Train Accuracy: 93.45%, Val Loss: 0.5689, Val Accuracy: 84.49%\n","[Epoch 5] Train Loss: 0.2028,  Train Accuracy: 93.60%, Val Loss: 0.6534, Val Accuracy: 82.74%\n","[Epoch 6] Train Loss: 0.1840,  Train Accuracy: 94.09%, Val Loss: 0.6785, Val Accuracy: 82.39%\n","[Epoch 7] Train Loss: 0.1799,  Train Accuracy: 94.22%, Val Loss: 0.7283, Val Accuracy: 80.80%\n","[Epoch 8] Train Loss: 0.1696,  Train Accuracy: 94.59%, Val Loss: 0.7433, Val Accuracy: 80.24%\n","[Epoch 9] Train Loss: 0.1700,  Train Accuracy: 94.61%, Val Loss: 0.7925, Val Accuracy: 79.85%\n","[Epoch 10] Train Loss: 0.1486,  Train Accuracy: 95.31%, Val Loss: 0.8560, Val Accuracy: 78.49%\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Parent directory /content/drive/MyDrive/FL/FL_Project_FedeExperiments/checkpoints does not exist.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-252ceccaf077>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# SALVATAGGIO CHECKPOINT -> contien stato dell'optimizer per riprendere l'addestramento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         torch.save({\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;34m\"model_state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# restituisce un dizionario Python che contiene tutti i parametri apprendibili del modello (pesi e bias)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m             _save(\n\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m             )\n\u001b[1;32m    780\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_compute_crc32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Parent directory /content/drive/MyDrive/FL/FL_Project_FedeExperiments/checkpoints does not exist."]}],"source":["start_epoch = epoch if epoch is not None else 1\n","num_epochs = wandb.config.epochs    # retrieve the desired number of training epochs you've previously specified in your WandB setup\n","checkpoint_interval = 10\n","\n","# TRAINING\n","for epoch in range(start_epoch, num_epochs + 1):\n","    model.train()     # Attiva modalità training\n","    train_loss = 0.0  # Reset ad ogni epoca\n","    correct_train = 0\n","    total_train = 0\n","\n","    # Loop sui Batch di Training\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device) # sposta i dati su gpu quando disponibile\n","        outputs = model(images)\n","        loss = loss_fn(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()    # backpropagation\n","        optimizer.step()   # aggiorna i pesi\n","\n","        train_loss += loss.item() * images.size(0) # serve per accumulare la loss ad ogni epoca\n","        _, predicted = torch.max(outputs, 1) # Ignora i valori massimi (_), e tiene solo gli indici (predicted) -> questo perchè poi confrontiamo gli indici con le labels\n","        total_train += labels.size(0) # Numero di immagini nel batch corrent\n","        correct_train += (predicted == labels).sum().item() # Crea un tensore booleano poi somma le TRUE e infine converte in un numero\n","\n","    train_accuracy = 100 * correct_train / total_train\n","    avg_train_loss = train_loss / total_train\n","\n","    #  VALIDAZIONE\n","    model.eval() # Attiva modalità di validation\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad(): # no calcolo gradienti\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device) # sposta i dati su gpu quando disponibile\n","            outputs = model(images)\n","\n","            # Calcolo loss\n","            loss = loss_fn(outputs, labels)\n","            val_loss += loss.item() * images.size(0) # images.size(0) restituisce la dimensione lungo il primo asse tipo numpy, dovrebbe variare con il batch size\n","                                                     # potremmo usare anche len(labels) ma con .size(0) gestiamo il caso in cui ultimo batch contenga un numero inferiore di osservazioni\n","\n","            # Calcolo accuratezza\n","            _, predicted = torch.max(outputs, 1) # Ignora i valori massimi (_), e tiene solo gli indici (predicted) -> questo perchè poi confrontiamo gli indici con le labels\n","            total += labels.size(0)   # Numero di immagini nel batch corrent\n","            correct += (predicted == labels).sum().item() # Crea un tensore booleano poi somma le TRUE e infine converte in un numero\n","\n","    avg_val_loss = val_loss / total  # Loss media\n","    val_accuracy = 100 * correct / total\n","\n","    # LOG SU W&B\n","    wandb.log({\n","        \"train_loss\": avg_train_loss,\n","        \"train_accuracy\": train_accuracy,\n","        \"val_loss\": avg_val_loss,\n","        \"val_accuracy\": val_accuracy,\n","        \"epoch\": epoch\n","    }, step=epoch)\n","\n","    print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f},  Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","\n","    # SALVATAGGIO CHECKPOINT -> contien stato dell'optimizer per riprendere l'addestramento\n","    if epoch % checkpoint_interval == 0:\n","        torch.save({\n","            \"epoch\": epoch,\n","            \"model_state_dict\": model.state_dict(),  # restituisce un dizionario Python che contiene tutti i parametri apprendibili del modello (pesi e bias)\n","            \"optimizer_state_dict\": optimizer.state_dict(),\n","            \"train_loss\": avg_train_loss,\n","            \"val_loss\": avg_val_loss,\n","        }, checkpoint_path)\n","        print(f\" Checkpoint salvato su Drive: {checkpoint_path}\")\n","\n","        #  (Opzionale) LOG ARTIFACT SU W&B -> dobbiamo capire se farlo o no, se i checkpoint sono molto grandi (es. >1GB), meglio salvarli localmente\n","        # potrebbe avere senso salvarci il miglior modello ma ora con solo il modello consigliato si pùò tralasciare\n","\n","        # artifact = wandb.Artifact(f\"{model_name}_checkpoint_ep{epoch}\", type=\"model\")\n","        # artifact.add_file(checkpoint_path)\n","        # wandb.log_artifact(artifact)\n","\n","wandb.finish()"]},{"cell_type":"code","source":["git = git_manager.GitManager()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pldIvxA6uJNv","executionInfo":{"status":"ok","timestamp":1745396351809,"user_tz":-120,"elapsed":1018,"user":{"displayName":"Niccolò","userId":"00690440481202077065"}},"outputId":"4d1d017b-4fd9-477b-c07b-ef37d9b7f217"},"id":"pldIvxA6uJNv","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["git.commit_and_push()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55nhJfqDutpV","outputId":"004d05d7-d9c6-4d57-b651-d5c1e409a596"},"id":"55nhJfqDutpV","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["🔑 Already logged in\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"jupytext":{"cell_metadata_filter":"-all","main_language":"python","notebook_metadata_filter":"-all"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}