{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27d1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import Subset, DataLoader, random_split\n",
    "\n",
    "from FederatedLearningProject.data.cifar100_loader import get_cifar100\n",
    "import FederatedLearningProject.checkpoints.checkpointing as checkpointing\n",
    "from FederatedLearningProject.training.centralized_training import train_and_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228da2b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "228da2b7",
    "outputId": "a38caf92-383e-4724-e3f3-f1ae0448b366"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "import shutil\n",
    "import os                              # Import the 'os' module for changing directories\n",
    "os.chdir('/content/drive/MyDrive/FL')  # Change the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usCBOVfNjyQI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usCBOVfNjyQI",
    "outputId": "4ba91a9b-324c-4608-82ee-1814bcf43c49"
   },
   "outputs": [],
   "source": [
    "wandb.login() # Ask for your API key for logging in to the wandb library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1uwQJOSKHgl",
   "metadata": {
    "id": "f1uwQJOSKHgl"
   },
   "outputs": [],
   "source": [
    "# Import CIFAR100 dataset: train_set, val_set, test_set\n",
    "# The transforms are applied before returning the dataset (in the module)\n",
    "\n",
    "valid_split_perc = 0.2    # of the 50000 training data\n",
    "train_set, val_set, test_set = get_cifar100(valid_split_perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c_UDv-rPjVSj",
   "metadata": {
    "id": "c_UDv-rPjVSj"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, validation, and test sets\n",
    "\n",
    "# batch_size è in hyperparameter (64, 128, ..), anche num_workers (consigliato per colab 2 o 4)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TaKru93QF_8C",
   "metadata": {
    "id": "TaKru93QF_8C"
   },
   "source": [
    "\n",
    "### Possible Models\n",
    "|                        | **Simple linear head**                               | **MLP head w/ Dropout**                                                      |\n",
    "| :--------------------- | :--------------------------------------------------- | :--------------------------------------------------------------------------- |\n",
    "| **Definition**         | `nn.Linear(384 → 100)`                               | `Dropout → Linear(384 → 256) → ReLU → Dropout → Linear(256 → 100)`           |\n",
    "| **# trainable params** | 384×100 + 100 ≈ **38 500**                           | 384×256+256 + 256×100+100 ≈ **123 000**                                      |\n",
    "| **Regularization**     | none                                                 | dropout on both layers                                                       |\n",
    "| **Expressive power**   | low  – just a single hyperplane on the CLS embedding | higher – small nonlinear bottleneck can learn more complex features in heads |\n",
    "| **Compute / memory**   | minimal                                              | \\~3× more weights, a bit more forward/backward cost                          |\n",
    "\n",
    "---\n",
    "\n",
    "**Appunto sui layer di testa:**\n",
    "\n",
    "1. **`self.classifier`**\n",
    "\n",
    "   * **Cosa contiene?** Un singolo `nn.Linear(embed_dim → num_classes)`.\n",
    "   * **Quando usarlo?** Se vuoi un *linear probe* puro: un solo layer che prende il CLS token e mappa direttamente alle classi.\n",
    "   * **Pro:** estremamente leggero (∼38 K parametri), veloce da addestrare e da inferire.\n",
    "   * **Contro:** capacità espressiva minima (è solo un’iper‐superficie lineare sullo spazio degli embedding).\n",
    "\n",
    "2. **`self.head`**\n",
    "\n",
    "   * **Cosa contiene?** Una piccola sequenza (`nn.Sequential`) di layer:\n",
    "\n",
    "     * Dropout\n",
    "     * Linear (embed\\_dim → hidden\\_dim)\n",
    "     * ReLU\n",
    "     * Dropout\n",
    "     * Linear (hidden\\_dim → num\\_classes)\n",
    "   * **Quando usarlo?** Se vuoi dare al tuo “probe” un po’ più di potenza di calcolo, trasformando non-linearmente il CLS prima della classificazione.\n",
    "   * **Pro:** maggiore capacità di apprendere rappresentazioni complesse nella testa, un minimo di regolarizzazione via dropout.\n",
    "   * **Contro:** più pesante (∼3× parametri in più rispetto al solo `classifier`), leggermente più lento da addestrare e inferire.\n",
    "\n",
    "---\n",
    "\n",
    "### Perché una piuttosto che l’altra?\n",
    "\n",
    "* **Vincoli di risorse** (GPU/RAM, tempo d’addestramento):\n",
    "\n",
    "  * Se sei sotto forte pressione computazionale o vuoi risultati rapidi, opti per `self.classifier`.\n",
    "* **Prestazioni** (accuratezza su dataset piccolo/mediamente grande come CIFAR-100):\n",
    "\n",
    "  * Se noti che il linear probe raggiunge un plateau basso, un piccolo MLP (`self.head`) può guadagnare qualche punto percentuale in più.\n",
    "* **Semplicità vs flessibilità**:\n",
    "\n",
    "  * Con una sola `classifier` hai un codice più pulito e diretto.\n",
    "  * Con `head` puoi sperimentare — cambiare `hidden_dim`, aggiungere altro dropout, batchnorm o ulteriori layer.\n",
    "\n",
    "In definitiva, **il nome** (`classifier` vs `head`) è arbitrario: serve a rendere più chiaro nel codice di che “peso” stiamo parlando. Se hai un solo layer, chiamalo `classifier`; se invece è un blocco più articolato, chiamalo `head` o `projection_head`, per tener separata la parte “feature extractor” (backbone) dalla parte “feature consumer” (testa di classificazione).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRRs8T8qCwcr",
   "metadata": {
    "id": "jRRs8T8qCwcr"
   },
   "outputs": [],
   "source": [
    "# # --- MLP head w/ Dropout ---\n",
    "# # Load DINO ViT-S/16 backbone and freeze it\n",
    "# backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "# backbone.eval()\n",
    "\n",
    "# for p in backbone.parameters():\n",
    "#     p.requires_grad = False\n",
    "\n",
    "# # Define the classifier head with optional dropout/MLP\n",
    "# class DinoClassifier(nn.Module):\n",
    "#     def __init__(self, backbone, num_classes=100, hidden_dim=256, drop=0.5):\n",
    "#         super().__init__()\n",
    "#         self.backbone = backbone\n",
    "#         embed_dim = backbone.embed_dim  # 384 for ViT-S/16\n",
    "#         self.head = nn.Sequential(\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(embed_dim, hidden_dim),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(drop),\n",
    "#             nn.Linear(hidden_dim, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # get CLS token from the frozen backbone\n",
    "#         with torch.no_grad():\n",
    "#             cls = self.backbone.forward_features(x)   # -> (B, embed_dim)\n",
    "#         return self.head(cls)\n",
    "\n",
    "# model = DinoClassifier(backbone, num_classes=100).to(device)\n",
    "\n",
    "# # ensure backbone stays in eval, head in train\n",
    "# model.backbone.eval()\n",
    "# model.head.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rLLuc8n9E92o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLLuc8n9E92o",
    "outputId": "03c340ce-81be-4872-dd98-1fc1e91d8470"
   },
   "outputs": [],
   "source": [
    "# --- Simple linear Head ---\n",
    "# Load DINO ViT-S/16 backbone e freeze\n",
    "backbone = torch.hub.load('facebookresearch/dino:main', 'dino_vits16')\n",
    "backbone.eval()\n",
    "\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Define the classifier head\n",
    "class DinoClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes=100):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        embed_dim = backbone.embed_dim  # 384 per ViT-S/16\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # estraggo CLS token con backbone congelato\n",
    "        with torch.no_grad():\n",
    "            cls = self.backbone.forward_features(x)  # -> (B, embed_dim)\n",
    "        return self.classifier(cls)\n",
    "\n",
    "model = DinoClassifier(backbone, num_classes=100)\n",
    "\n",
    "# Device selection: Use GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x3NlXi1Y-PB2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3NlXi1Y-PB2",
    "outputId": "cc9a9cbb-546e-43a6-92d7-9f10358e3860"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a258a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A-GWt2WqlJyY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-GWt2WqlJyY",
    "outputId": "079862bb-1864-4ef2-863c-4c2ebf3b4a49"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RqiWVe89NEJC",
   "metadata": {
    "id": "RqiWVe89NEJC"
   },
   "outputs": [],
   "source": [
    "# --- OPTIMIZER AND LOSS FUNCTION ---\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4)  # momentum=0.9, weight_decay=5e-4 -> optimizer consigliato\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w2WnTA_ue2sX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "w2WnTA_ue2sX",
    "outputId": "8e1e6e0e-9af4-455b-a6b3-319a2a6e5e2c"
   },
   "outputs": [],
   "source": [
    "# wandb.init() prepares the tracking of hyperparameters/metrics for later recording performance using wandb.log()\n",
    "\n",
    "model_name = \"dino_vits16\"\n",
    "project_name = \"FederatedProject\"\n",
    "run_name = f\"{model_name}_run\"\n",
    "\n",
    "# INITIALIZE W&B\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=run_name,\n",
    "    config={\n",
    "        \"model\": model_name,\n",
    "        \"epochs\": 50,\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"architecture\": model.__class__.__name__,\n",
    "})\n",
    "\n",
    "# Copy your config\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mo4lpz6c3C-F",
   "metadata": {
    "id": "mo4lpz6c3C-F"
   },
   "outputs": [],
   "source": [
    "#  PERCORSO CHECKPOINT\n",
    "checkpoint_dir = \"/content/drive/MyDrive/FL/FederatedLearningProject/checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, f\"{model_name}_checkpoint.pth\")    # we predefine the name of the file inside the specified folder (dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cBdBMnxzyusN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBdBMnxzyusN",
    "outputId": "94377154-5f2e-45cf-885d-16e47994b887"
   },
   "outputs": [],
   "source": [
    "# RECOVER CHECKPOINT\n",
    "start_epoch, model_data = checkpointing.load_checkpoint(model, optimizer, checkpoint_dir)\n",
    "\n",
    "try:\n",
    "  print()\n",
    "  print(f\"The 'model_data' dictionary contains the following keys: {list(model_data.keys())}\")\n",
    "  model.load_state_dict(model_data[\"model_state_dict\"])\n",
    "  optimizer.load_state_dict(model_data[\"optimizer_state_dict\"])\n",
    "except: None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ZGaA_bF5Cpy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "3ZGaA_bF5Cpy",
    "outputId": "b34711c9-d953-4ed3-c4ee-69b2cc75d1cd"
   },
   "outputs": [],
   "source": [
    "# --- TRAINING LOOP ---\n",
    "# Call to the training loop function\n",
    "train_and_validate(start_epoch, model, train_loader, val_loader, optimizer, criterion, device, checkpoint_path, num_epochs=20, checkpoint_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lCT7l21Lpl9L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "lCT7l21Lpl9L",
    "outputId": "8c441b3d-9224-43fe-9113-c311bb43139b"
   },
   "outputs": [],
   "source": [
    "## Display some informations ##\n",
    "\n",
    "print(\"Model:\", model_name)\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Batch size:\", train_loader.batch_size)\n",
    "print(\"Number of epochs:\", config.epochs)\n",
    "print(\"DataLoader: \")\n",
    "print(\"Learning rate:\", optimizer.param_groups[0]['lr'])\n",
    "print(\"Architecture:\", model.__class__.__name__)\n",
    "print(\"Device:\", device)\n",
    "print(\"Optimizer:\", optimizer)\n",
    "print(\"Loss function:\", criterion)\n",
    "print(\"Checkpoint directory:\", checkpoint_dir)\n",
    "print(\"Checkpoint path:\", checkpoint_path)\n",
    "print(\"Current epoch:\", epoch)\n",
    "print()\n",
    "\n",
    "print(\"Train Loader Information:\")\n",
    "print(f\"  Number of batches: {len(train_loader)}\")\n",
    "print(f\"  Batch size: {train_loader.batch_size}\")\n",
    "# Get the dimension of a single batch\n",
    "for images, labels in train_loader:\n",
    "  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n",
    "  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n",
    "  break  # Exit the loop after processing one batch\n",
    "print()\n",
    "\n",
    "print(\"\\nValidation Loader Information:\")\n",
    "print(f\"  Number of batches: {len(val_loader)}\")\n",
    "print(f\"  Batch size: {val_loader.batch_size}\")\n",
    "# Get the dimension of a single batch\n",
    "for images, labels in val_loader:\n",
    "  print(f\"  Dimension of 1 batch (images): {images.shape}\")\n",
    "  print(f\"  Dimension of 1 batch (labels): {labels.shape}\")\n",
    "  break  # Exit the loop after processing one batch\n",
    "print()\n",
    "\n",
    "# Check for CUDA availability\n",
    "print(\"CUDA AVAIABILITY:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# Print model architecture summary\n",
    "print(\"\\nMODEL ARCHITECTURE:\")\n",
    "print(model)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
